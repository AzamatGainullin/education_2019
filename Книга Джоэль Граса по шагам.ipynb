{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#import pylab as plt\n",
    "#import networkx as nx\n",
    "import os\n",
    "#os.getcwd()\n",
    "import math, random, re, datetime\n",
    "from collections import defaultdict, Counter, deque\n",
    "import re, math, random # регулярные выражения, математические функции и случайные числа\n",
    "\n",
    "import glob\n",
    "from lib.naive_bayes import tokenize\n",
    "\n",
    "\n",
    "\n",
    "from functools import partial, reduce\n",
    "\n",
    "#from lib.linear_algebra import dot, get_row, get_column, make_matrix, \\\n",
    "                              # magnitude, scalar_multiply, shape, distance\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../code-python3-ru\")\n",
    "from lib.statistics import median\n",
    "from lib.machine_learning import split_data\n",
    "from lib.gradient_descent import minimize_stochastic\n",
    "from lib.simple_linear_regression import total_sum_of_squares\n",
    "\n",
    "from lib.linear_algebra import sum_of_squares, distance\n",
    "from lib.linear_algebra import squared_distance, vector_mean\n",
    "\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "%matplotlib inline \n",
    "\n",
    "\n",
    "\n",
    "from lib.linear_algebra import shape, get_row, get_column, make_matrix, \\\n",
    "     vector_mean, vector_sum, dot, magnitude, vector_subtract, scalar_multiply\n",
    "from lib.statistics import correlation, standard_deviation, mean\n",
    "from lib.probability import inverse_normal_cdf\n",
    "from lib.gradient_descent import maximize_batch\n",
    "\n",
    "import csv\n",
    "\n",
    "import dateutil.parser\n",
    "from bs4 import BeautifulSoup  # установить pip (pip3) install BeautifulSoup\n",
    "import requests\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# АНАЛИЗ СОЦИАЛЬНЫХ СЕТЕЙ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interests = [\n",
    "    (0, \"Hadoop\"), (0, \"Big Data\"), (0, \"HBase\"), (0, \"Java\"),\n",
    "    (0, \"Spark\"), (0, \"Storm\"), (0, \"Cassandra\"),\n",
    "    (1, \"NoSQL\"), (1, \"MongoDB\"), (1, \"Cassandra\"), (1, \"HBase\"),\n",
    "    (1, \"Postgres\"), (2, \"Python\"), (2, \"scikit-learn\"), (2, \"scipy\"),\n",
    "    (2, \"numpy\"), (2, \"statsmodels\"), (2, \"pandas\"), (3, \"R\"), (3, \"Python\"),\n",
    "    (3, \"statistics\"), (3, \"regression\"), (3, \"probability\"),\n",
    "    (4, \"machine learning\"), (4, \"regression\"), (4, \"decision trees\"),\n",
    "    (4, \"libsvm\"), (5, \"Python\"), (5, \"R\"), (5, \"Java\"), (5, \"C++\"),\n",
    "    (5, \"Haskell\"), (5, \"programming languages\"), (6, \"statistics\"),\n",
    "    (6, \"probability\"), (6, \"mathematics\"), (6, \"theory\"),\n",
    "    (7, \"machine learning\"), (7, \"scikit-learn\"), (7, \"Mahout\"),\n",
    "    (7, \"neural networks\"), (8, \"neural networks\"), (8, \"deep learning\"),\n",
    "    (8, \"Big Data\"), (8, \"artificial intelligence\"), (9, \"Hadoop\"),\n",
    "    (9, \"Java\"), (9, \"MapReduce\"), (9, \"Big Data\")]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "users = [\n",
    "    { \"id\": 0, \"name\": \"Hero\" },\n",
    "    { \"id\": 1, \"name\": \"Dunn\" },\n",
    "    { \"id\": 2, \"name\": \"Sue\" },\n",
    "    { \"id\": 3, \"name\": \"Chi\" },\n",
    "    { \"id\": 4, \"name\": \"Thor\" },\n",
    "    { \"id\": 5, \"name\": \"Clive\" },\n",
    "    { \"id\": 6, \"name\": \"Hicks\" },\n",
    "    { \"id\": 7, \"name\": \"Devin\" },\n",
    "    { \"id\": 8, \"name\": \"Kate\" },\n",
    "    { \"id\": 9, \"name\": \"Klein\" }\n",
    "]\n",
    "\n",
    "friendships = [(0, 1), (0, 2), (1, 2), (1, 3), (2, 3), (3, 4),\n",
    "               (4, 5), (5, 6), (5, 7), (6, 8), (7, 8), (8, 9)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for user in users:\n",
    "    user['friends']=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,j in friendships:\n",
    "    users[i]['friends'].append(users[j])\n",
    "    users[j]['friends'].append(users[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def number_of_friends(user):\n",
    "    return len(user['friends'])\n",
    "total_connections=sum(number_of_friends(user) for user in users)\n",
    "total_connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_users=len(users)\n",
    "avg_connections=total_connections/num_users\n",
    "avg_connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_friends_by_id=[(user['id'],number_of_friends(user)) for user in users]\n",
    "num_friends_by_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(num_friends_by_id, key=lambda pair: pair[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def friends_of_friend_ids_bad(user):\n",
    "    # \"foaf\" - это аббревиатура для \"friend of a friend\", .т.е. друг конкретного друга\n",
    "    return [foaf[\"id\"] for friend in user[\"friends\"] for foaf in friend[\"friends\"]] # получить всех ЕГО друзей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def not_the_same(user, other_user):\n",
    "    return user['id']!=other_user['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def not_friends(user, other_user):\n",
    "    return all(not_the_same(friend, other_user) for friend in user['friends'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def friends_of_friend_ids(user):\n",
    "    return Counter(foaf[\"id\"]\n",
    "                   for friend in user[\"friends\"]  # для каждого моего друга\n",
    "                   for foaf in friend[\"friends\"]  # подсчитать ИХ друзей,\n",
    "                   if not_the_same(user, foaf)    # которые не являются мной\n",
    "                   and not_friends(user, foaf))   # и не мои друзья\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for user in users:\n",
    "    print(friends_of_friend_ids(user))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_scientists_who_like(target_interest):\n",
    "    return [user_id for user_id, user_interest in interests\n",
    "           if user_interest==target_interest]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_scientists_who_like(\"Big Data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_ids_by_interest=defaultdict(list)\n",
    "user_ids_by_interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for user_id, interest in interests:\n",
    "    user_ids_by_interest[interest].append(user_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_ids_by_interest['Big Data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interests_by_user_id=defaultdict(list)\n",
    "for user_id, interest in interests:\n",
    "    interests_by_user_id[user_id].append(interest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interests_by_user_id[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_common_interests_with(user):\n",
    "    return Counter(interested_user_id\n",
    "        for interest in interests_by_user_id[user['id']]\n",
    "        for interested_user_id in user_ids_by_interest[interest]\n",
    "        if interested_user_id != user['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_common_interests_with(users[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "salaries_and_tenures = [(83000, 8.7), (88000, 8.1),\n",
    "                        (48000, 0.7), (76000, 6),\n",
    "                        (69000, 6.5), (76000, 7.5),\n",
    "                        (60000, 2.5), (83000, 10),\n",
    "                        (48000, 1.9), (63000, 4.2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "salary_by_tenure=defaultdict(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for salary, tenure in salaries_and_tenures:\n",
    "    salary_by_tenure[tenure].append(salary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_salary_by_tenure={tenure: sum(salaries)/len(salaries)\n",
    "                          for tenure, salaries in salary_by_tenure.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_salary_by_tenure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tenure_bucket(tenure):\n",
    "    if tenure<2:\n",
    "        return \"менее двух\"\n",
    "    elif tenure<5:\n",
    "        return \"между 2 и 5\"\n",
    "    else:\n",
    "        return \"более пяти\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "salary_by_tenure_bucket=defaultdict(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for salary, tenure in salaries_and_tenures:\n",
    "    bucket=tenure_bucket(tenure)\n",
    "    salary_by_tenure_bucket[bucket].append(salary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "salary_by_tenure_bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_salary_by_bucket = {\n",
    "  tenure_bucket : sum(salaries) / len(salaries)\n",
    "  for tenure_bucket, salaries in salary_by_tenure_bucket.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_salary_by_bucket"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# круто. ведь можно написать последний абзац кода не только для средней зарплаты, но и для максимальной в каждом интервале и т.п."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_and_counts=Counter(word for user, interest in interests\n",
    "                        for word in interest.lower().split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_and_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interest.lower().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word, count in words_and_counts.most_common():\n",
    "    if count>1:\n",
    "        print(word, count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "two_plus_three=2+\\\n",
    "                        3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "two_plus_three"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [1,2,3,4,5]:\n",
    "    \n",
    "    \n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lazy_range(n):\n",
    "    i=0\n",
    "    while i < n:\n",
    "        yield i\n",
    "        i=i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in lazy_range(5):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#\n",
    "# функции для работы с векторами\n",
    "#\n",
    "\n",
    "def vector_add(v, w):\n",
    "    \"\"\"покомпонентное сложение двух векторов\"\"\"\n",
    "    return [v_i + w_i for v_i, w_i in zip(v,w)]\n",
    "\n",
    "def vector_subtract(v, w):\n",
    "    \"\"\"покомпонентное вычитание двух векторов\"\"\"\n",
    "    return [v_i - w_i for v_i, w_i in zip(v,w)]\n",
    "\n",
    "def vector_sum(vectors):\n",
    "    return reduce(vector_add, vectors)\n",
    "\n",
    "def scalar_multiply(c, v):\n",
    "    return [c * v_i for v_i in v]\n",
    "\n",
    "def vector_mean(vectors):\n",
    "    \"\"\"вычислить вектор, чей i-й элемент - это среднее значение\n",
    "    всех i-х элементов входящих векторов\"\"\"\n",
    "    n = len(vectors)\n",
    "    return scalar_multiply(1/n, vector_sum(vectors))\n",
    "\n",
    "def dot(v, w):\n",
    "    \"\"\"v_1 * w_1 + ... + v_n * w_n\"\"\"\n",
    "    return sum(v_i * w_i for v_i, w_i in zip(v, w))\n",
    "\n",
    "def sum_of_squares(v):\n",
    "    \"\"\"v_1 * v_1 + ... + v_n * v_n\"\"\"\n",
    "    return dot(v, v)\n",
    "\n",
    "def magnitude(v):\n",
    "    return math.sqrt(sum_of_squares(v))\n",
    "\n",
    "def squared_distance(v, w):\n",
    "    return sum_of_squares(vector_subtract(v, w))\n",
    "\n",
    "def distance(v, w):\n",
    "    return math.sqrt(squared_distance(v, w))\n",
    "\n",
    "#\n",
    "# функции для работы с матрицами\n",
    "#\n",
    "\n",
    "def shape(A):\n",
    "    num_rows = len(A)\n",
    "    num_cols = len(A[0]) if A else 0\n",
    "    return num_rows, num_cols\n",
    "\n",
    "def get_row(A, i):\n",
    "    return A[i]\n",
    "\n",
    "def get_column(A, j):\n",
    "    return [A_i[j] for A_i in A]\n",
    "\n",
    "def make_matrix(num_rows, num_cols, entry_fn):\n",
    "    \"\"\"возвращает матрицу размером num_rows x num_cols,\n",
    "    (i,j)-й элемент которой равен функции entry_fn(i, j)\"\"\"\n",
    "    return [[entry_fn(i, j) for j in range(num_cols)]\n",
    "            for i in range(num_rows)]\n",
    "\n",
    "def is_diagonal(i, j):\n",
    "    \"\"\"единицы по диагонали, остальные нули\"\"\"\n",
    "    return 1 if i == j else 0\n",
    "\n",
    "identity_matrix = make_matrix(5, 5, is_diagonal)\n",
    "\n",
    "# пользователь  0  1  2  3  4  5  6  7  8  9\n",
    "#\n",
    "friendships = [[0, 1, 1, 0, 0, 0, 0, 0, 0, 0], # пользователь 0\n",
    "               [1, 0, 1, 1, 0, 0, 0, 0, 0, 0], # пользователь 1\n",
    "               [1, 1, 0, 1, 0, 0, 0, 0, 0, 0], # пользователь 2\n",
    "               [0, 1, 1, 0, 1, 0, 0, 0, 0, 0], # пользователь 3\n",
    "               [0, 0, 0, 1, 0, 1, 0, 0, 0, 0], # пользователь 4\n",
    "               [0, 0, 0, 0, 1, 0, 1, 1, 0, 0], # пользователь 5\n",
    "               [0, 0, 0, 0, 0, 1, 0, 0, 1, 0], # пользователь 6\n",
    "               [0, 0, 0, 0, 0, 1, 0, 0, 1, 0], # пользователь 7\n",
    "               [0, 0, 0, 0, 0, 0, 1, 1, 0, 1], # пользователь 8\n",
    "               [0, 0, 0, 0, 0, 0, 0, 0, 1, 0]] # пользователь 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "friends_of_five=[i for i, is_friend in enumerate(friendships[5]) if is_friend]\n",
    "friends_of_five"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, is_friend in enumerate(friendships[5]):\n",
    "    print(i, is_friend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entry_fn(x,y):\n",
    "    if y!=0:\n",
    "        return round(x/y,1)\n",
    "    elif x!=0:\n",
    "        return round(y/x,1)\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "make_matrix(5, 5, entry_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "users = [\n",
    "    { \"id\": 0, \"name\": \"Hero\" },\n",
    "    { \"id\": 1, \"name\": \"Dunn\" },\n",
    "    { \"id\": 2, \"name\": \"Sue\" },\n",
    "    { \"id\": 3, \"name\": \"Chi\" },\n",
    "    { \"id\": 4, \"name\": \"Thor\" },\n",
    "    { \"id\": 5, \"name\": \"Clive\" },\n",
    "    { \"id\": 6, \"name\": \"Hicks\" },\n",
    "    { \"id\": 7, \"name\": \"Devin\" },\n",
    "    { \"id\": 8, \"name\": \"Kate\" },\n",
    "    { \"id\": 9, \"name\": \"Klein\" }\n",
    "]\n",
    "\n",
    "friendships = [(0, 1), (0, 2), (1, 2), (1, 3), (2, 3), (3, 4),\n",
    "               (4, 5), (5, 6), (5, 7), (6, 8), (7, 8), (8, 9)]\n",
    "\n",
    "# предоставить каждому пользователю пустой список друзей\n",
    "for user in users:\n",
    "    user[\"friends\"] = []\n",
    "\n",
    "# и заполнить его \n",
    "for i, j in friendships:\n",
    "    # это работает, потому что users[i] - это пользователь, чей id равен i\n",
    "    users[i][\"friends\"].append(users[j]) # добавить j как друга для i\n",
    "    users[j][\"friends\"].append(users[i]) # добавить i как друга для j\n",
    "\n",
    "#\n",
    "# Центральность по посредничеству\n",
    "#\n",
    "\n",
    "def shortest_paths_from(from_user):\n",
    "\n",
    "    # словарь из \"user_id\" до ВСЕХ кратчайших путей к этому пользователю\n",
    "    shortest_paths_to = { from_user[\"id\"] : [[]] }\n",
    "\n",
    "    # двусвязная очередь (предыдущий пользователь, следующий пользователь),\n",
    "    # с которой нужно сверяться; инициализируется всеми парами,\n",
    "    # состоящими из исходного пользователя\n",
    "    # и его друзьями (from_user, friend_of_from_user)\n",
    "    frontier = deque((from_user, friend)\n",
    "                     for friend in from_user[\"friends\"])\n",
    "\n",
    "    # продолжать, пока очередь не пуста\n",
    "    while frontier:\n",
    "\n",
    "        prev_user, user = frontier.popleft() # take from the beginning\n",
    "        user_id = user[\"id\"]\n",
    "\n",
    "        # в силу особенности добавления элементов к очереди с неизбежностью\n",
    "        # некоторые из кратчайших путей к prev_user уже известны\n",
    "        paths_to_prev = shortest_paths_to[prev_user[\"id\"]]\n",
    "        paths_via_prev = [path + [user_id] for path in paths_to_prev]\n",
    "\n",
    "        # возможно, кратчайший путь уже известен\n",
    "        old_paths_to_here = shortest_paths_to.get(user_id, [])\n",
    "\n",
    "        # каков кратчайший путь до этого места, которое уже встречалось ранее?\n",
    "        if old_paths_to_here:\n",
    "            min_path_length = len(old_paths_to_here[0])\n",
    "        else:\n",
    "            min_path_length = float('inf')\n",
    "\n",
    "        # хранить только пути, которые не слишком длинные\n",
    "        # и действительно новые\n",
    "        new_paths_to_here = [path_via_prev\n",
    "                             for path_via_prev in paths_via_prev\n",
    "                             if len(path_via_prev) <= min_path_length\n",
    "                             and path_via_prev not in old_paths_to_here]\n",
    "\n",
    "        shortest_paths_to[user_id] = old_paths_to_here + new_paths_to_here\n",
    "\n",
    "        # добавить к очереди frontier не встречавшихся ранее соседей\n",
    "        frontier.extend((user, friend)\n",
    "                        for friend in user[\"friends\"]\n",
    "                        if friend[\"id\"] not in shortest_paths_to)\n",
    "\n",
    "    return shortest_paths_to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for user in users:\n",
    "    user['shortest_paths'] = shortest_paths_from(user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users[5]['shortest_paths']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users[5]['friends'][0]['name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for user in users:\n",
    "    user['betweenness_centrality']=0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for source in users:\n",
    "    source_id = source['id']\n",
    "    for target_id, paths in source['shortest_paths'].items():\n",
    "        if source_id < target_id:\n",
    "            num_paths = len(paths)\n",
    "            contrib=1 / num_paths\n",
    "            for path in paths:\n",
    "                for id in path:\n",
    "                    if id not in [source_id, target_id]:\n",
    "                        users[id]['betweenness_centrality']=users[id]['betweenness_centrality']+contrib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for user in users:\n",
    "    print(user['id'], user['betweenness_centrality'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def farness(user):\n",
    "    return sum(len(paths[0]) for paths in user['shortest_paths'].values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "users[5]['shortest_paths'].values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for user in users:\n",
    "    user['closeness_centrality'] = 1 / farness(user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for user in users:\n",
    "    print(user['id'], user['closeness_centrality'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# умножение матриц\n",
    "#\n",
    "\n",
    "def matrix_product_entry(A, B, i, j):\n",
    "    return dot(get_row(A, i), get_column(B, j))\n",
    "\n",
    "def matrix_multiply(A, B):\n",
    "    n1, k1 = shape(A)\n",
    "    n2, k2 = shape(B)\n",
    "    if k1 != n2:\n",
    "        raise ArithmeticError(\"несовместимые формы матриц!\")\n",
    "\n",
    "    return make_matrix(n1, k2, partial(matrix_product_entry, A, B))\n",
    "\n",
    "def vector_as_matrix(v):\n",
    "    \"\"\"возвращает n x 1 матричное представление\n",
    "    для вектора v (представленного списком)\"\"\"\n",
    "    return [[v_i] for v_i in v]\n",
    "\n",
    "def vector_from_matrix(v_as_matrix):\n",
    "    \"\"\"возвращает векторное представление в виде списка значений\n",
    "    для n x 1 матрицы\"\"\"\n",
    "    return [row[0] for row in v_as_matrix]\n",
    "\n",
    "def matrix_operate(A, v):\n",
    "    v_as_matrix = vector_as_matrix(v)\n",
    "    product = matrix_multiply(A, v_as_matrix)\n",
    "    return vector_from_matrix(product)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_eigenvector(A, tolerance = 0.00001):\n",
    "    guess = [random.random() for _ in A]\n",
    "    count = 0\n",
    "    while True:\n",
    "        result = matrix_operate(A, guess)\n",
    "        length = magnitude(result)\n",
    "        next_guess = scalar_multiply(1/length, result)\n",
    "        #count = count +1\n",
    "        \n",
    "        if distance(guess,next_guess) < tolerance:\n",
    "            return next_guess, length #, count\n",
    "        guess = next_guess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AA=[[-1,-6],[2,6]]\n",
    "find_eigenvector(AA, tolerance = 0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "friendships = [(0, 1), (0, 2), (1, 2), (1, 3), (2, 3), (3, 4),\n",
    "               (4, 5), (5, 6), (5, 7), (6, 8), (7, 8), (8, 9)]\n",
    "\n",
    "def entry_fn(i, j):\n",
    "    return 1 if (i, j) in friendships or (j, i) in friendships else 0\n",
    "\n",
    "n = len(users)\n",
    "adjacency_matrix = make_matrix(n, n, entry_fn)\n",
    "\n",
    "eigenvector_centralities, _ = find_eigenvector(adjacency_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eigenvector_centralities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adjacency_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endorsements = [(0, 1), (1, 0), (0, 2), (2, 0), (1, 2), (2, 1), (1, 3),\n",
    "                (2, 3), (3, 4), (5, 4), (5, 6), (7, 5), (6, 8), (8, 7), (8, 9)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for user in users:\n",
    "    user[\"endorses\"] = []       # этот список отслеживает исходящие оценки\n",
    "    user[\"endorsed_by\"] = []    # этот список отслеживает поступающие оценки\n",
    "\n",
    "for source_id, target_id in endorsements:\n",
    "    users[source_id][\"endorses\"].append(users[target_id])\n",
    "    users[target_id][\"endorsed_by\"].append(users[source_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endorsements_by_id = [(user[\"id\"], len(user[\"endorsed_by\"]))\n",
    "                      for user in users]\n",
    "\n",
    "sorted(endorsements_by_id,\n",
    "       key=lambda pair: pair[1],\n",
    "       reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def page_rank(users, damping = 0.85, num_iters = 100):\n",
    "\n",
    "    # первоначально распределить PageRank равномерно\n",
    "    num_users = len(users)\n",
    "    pr = { user[\"id\"] : 1 / num_users for user in users }\n",
    "\n",
    "    # это малая доля индекса PageRank,\n",
    "    # которую каждый узел получает на каждой итерации\n",
    "    base_pr = (1 - damping) / num_users\n",
    "\n",
    "    for __ in range(num_iters):\n",
    "        next_pr = { user[\"id\"] : base_pr for user in users }\n",
    "        for user in users:\n",
    "            # распределить PageRank среди исходящих связей\n",
    "            links_pr = pr[user[\"id\"]] * damping\n",
    "            for endorsee in user[\"endorses\"]:\n",
    "                next_pr[endorsee[\"id\"]] += links_pr / len(user[\"endorses\"])\n",
    "\n",
    "        pr = next_pr\n",
    "\n",
    "    return pr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_rank(users, damping = 0.85, num_iters = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# анализ своего апи в фейсбук, друзей и друзей друзей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = read_edgelist(\"facebook_combined.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd_file=pd.read_csv(\"facebook_combined.txt\")\n",
    "pd_file.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g.number_of_nodes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g.number_of_edges()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.draw(g)\n",
    "plt.savefig('graph.png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# СТАТИСТИКА"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_friends = [100,49,41,40,25,21,21,19,19,18,\n",
    "               18,16,15,15,15,15,14,14,13,13,\n",
    "               13,13,12,12,11,10,10,10,10,10,\n",
    "               10,10,10,10,10,10,10,10,10,10,\n",
    "               9,9,9,9,9,9,9,9,9,9,\n",
    "               9,9,9,9,9,9,9,9,8,8,\n",
    "               8,8,8,8,8,8,8,8,8,8,\n",
    "               8,7,7,7,7,7,7,7,7,7,\n",
    "               7,7,7,7,7,7,6,6,6,6,\n",
    "               6,6,6,6,6,6,6,6,6,6,\n",
    "               6,6,6,6,6,6,6,6,5,5,\n",
    "               5,5,5,5,5,5,5,5,5,5,\n",
    "               5,5,5,5,5,4,4,4,4,4,\n",
    "               4,4,4,4,4,4,4,4,4,4,\n",
    "               4,4,4,4,4,3,3,3,3,3,\n",
    "               3,3,3,3,3,3,3,3,3,3,\n",
    "               3,3,3,3,3,2,2,2,2,2,\n",
    "               2,2,2,2,2,2,2,2,2,2,\n",
    "               2,2,1,1,1,1,1,1,1,1,\n",
    "               1,1,1,1,1,1,1,1,1,1,\n",
    "               1,1,1,1]\n",
    "\n",
    "def make_friend_counts_histogram(plt):\n",
    "    friend_counts = Counter(num_friends)\n",
    "    xs = range(101)\n",
    "    ys = [friend_counts[x] for x in xs]\n",
    "    plt.bar(xs, ys)\n",
    "    plt.axis([0,101,0,25])\n",
    "    plt.title(\"Гистограмма количества друзей\")\n",
    "    plt.xlabel(\"Количество друзей\")\n",
    "    plt.ylabel(\"Количество людей\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_friend_counts_histogram(plt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_points = len(num_friends)               # 204\n",
    "\n",
    "largest_value = max(num_friends)            # 100\n",
    "smallest_value = min(num_friends)           # 1\n",
    "\n",
    "sorted_values = sorted(num_friends)\n",
    "smallest_value = sorted_values[0]           # 1\n",
    "second_smallest_value = sorted_values[1]    # 1\n",
    "second_largest_value = sorted_values[-2]    # 49"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_largest_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean(x):\n",
    "    return sum(x) / len(x)\n",
    "\n",
    "def median(v):\n",
    "    \"\"\"возвращает ближайшее к середине значение для v\"\"\"\n",
    "    n = len(v)\n",
    "    sorted_v = sorted(v)\n",
    "    midpoint = n // 2    # индекс серединного значения\n",
    "\n",
    "    if n % 2 == 1:\n",
    "        # если нечетное, вернуть серединное значение\n",
    "        return sorted_v[midpoint]\n",
    "    else:\n",
    "        # если четное, вернуть среднее 2-х серединных значений\n",
    "        lo = midpoint - 1\n",
    "        hi = midpoint\n",
    "        return (sorted_v[lo] + sorted_v[hi]) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean(num_friends)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "median(num_friends)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantile(x, p):\n",
    "    \"\"\"возвращает значение в x, соответствующее p-ому проценту данных\"\"\"\n",
    "    p_index = int(p * len(x))\n",
    "    return sorted(x)[p_index]\n",
    "\n",
    "def mode(x):\n",
    "    \"\"\"возвращает список, поскольку мод может быть больше одной\"\"\"\n",
    "    counts = Counter(x)\n",
    "    max_count = max(counts.values())\n",
    "    return [x_i for x_i, count in counts.items()\n",
    "            if count == max_count]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantile(num_friends, 0.90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode(num_friends)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(num_friends)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_range(x):\n",
    "    return max(x) - min(x)\n",
    "\n",
    "def de_mean(x):\n",
    "    \"\"\"пересчитать x, вычтя его среднее (среднее результата будет = 0)\"\"\"\n",
    "    x_bar = mean(x)\n",
    "    return [x_i - x_bar for x_i in x]\n",
    "\n",
    "def variance(x):\n",
    "    \"\"\"предполагается, что вектор x состоит как минимум из 2х элементов\"\"\"\n",
    "    n = len(x)\n",
    "    deviations = de_mean(x)\n",
    "    return sum_of_squares(deviations) / (n - 1)\n",
    "\n",
    "def standard_deviation(x):\n",
    "    return math.sqrt(variance(x))\n",
    "\n",
    "def interquartile_range(x):\n",
    "    return quantile(x, 0.75) - quantile(x, 0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variance(num_friends)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "standard_deviation(num_friends)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interquartile_range(num_friends)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####\n",
    "#\n",
    "# КОРРЕЛЯЦИЯ\n",
    "#\n",
    "#####\n",
    "\n",
    "# ежедневные минуты - сколько минут в день каждый пользователь тратит на DataSciencester\n",
    "daily_minutes = [1,68.77,51.25,52.08,38.36,44.54,57.13,51.4,41.42,31.22,\n",
    "                 34.76,54.01,38.79,47.59,49.1,27.66,41.03,36.73,48.65,28.12,\n",
    "                 46.62,35.57,32.98,35,26.07,23.77,39.73,40.57,31.65,31.21,\n",
    "                 36.32,20.45,21.93,26.02,27.34,23.49,46.94,30.5,33.8,24.23,\n",
    "                 21.4,27.94,32.24,40.57,25.07,19.42,22.39,18.42,46.96,23.72,\n",
    "                 26.41,26.97,36.76,40.32,35.02,29.47,30.2,31,38.11,38.18,\n",
    "                 36.31,21.03,30.86,36.07,28.66,29.08,37.28,15.28,24.17,22.31,\n",
    "                 30.17,25.53,19.85,35.37,44.6,17.23,13.47,26.33,35.02,32.09,\n",
    "                 24.81,19.33,28.77,24.26,31.98,25.73,24.86,16.28,34.51,15.23,\n",
    "                 39.72,40.8,26.06,35.76,34.76,16.13,44.04,18.03,19.65,32.62,\n",
    "                 35.59,39.43,14.18,35.24,40.13,41.82,35.45,36.07,43.67,24.61,\n",
    "                 20.9,21.9,18.79,27.61,27.21,26.61,29.77,20.59,27.53,13.82,\n",
    "                 33.2,25,33.1,36.65,18.63,14.87,22.2,36.81,25.53,24.62,\n",
    "                 26.25,18.21,28.08,19.42,29.79,32.8,35.99,28.32,27.79,35.88,\n",
    "                 29.06,36.28,14.1,36.63,37.49,26.9,18.58,38.48,24.48,18.95,\n",
    "                 33.55,14.24,29.04,32.51,25.63,22.22,19,32.73,15.16,13.9,\n",
    "                 27.2,32.01,29.27,33,13.74,20.42,27.32,18.23,35.35,28.48,\n",
    "                 9.08,24.62,20.12,35.26,19.92,31.02,16.49,12.16,30.7,31.22,\n",
    "                 34.65,13.13,27.51,33.2,31.57,14.1,33.42,17.44,10.12,24.42,\n",
    "                 9.82,23.39,30.93,15.03,21.67,31.09,33.29,22.61,26.89,23.48,\n",
    "                 8.38,27.81,32.35,23.84]\n",
    "\n",
    "def covariance(x, y):\n",
    "    n = len(x)\n",
    "    return dot(de_mean(x), de_mean(y)) / (n - 1)\n",
    "\n",
    "def correlation(x, y):\n",
    "    stdev_x = standard_deviation(x)\n",
    "    stdev_y = standard_deviation(y)\n",
    "    if stdev_x > 0 and stdev_y > 0:\n",
    "        return covariance(x, y) / stdev_x / stdev_y\n",
    "    else:\n",
    "        return 0 # если переменные не меняются, то корреляция равна нулю"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation(num_friends, daily_minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.corrcoef(num_friends, daily_minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier = num_friends.index(100) # индекс выброса\n",
    "\n",
    "num_friends_good = [x\n",
    "                    for i, x in enumerate(num_friends)\n",
    "                    if i != outlier]\n",
    "\n",
    "daily_minutes_good = [x\n",
    "                      for i, x in enumerate(daily_minutes)\n",
    "                      if i != outlier]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "correlation(num_friends_good, daily_minutes_good)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ТЕОРИЯ ВЕРОЯТНОСТЕЙ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_kid():\n",
    "    return random.choice(['boy','girl'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_kid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "both_girls=0\n",
    "older_girl=0\n",
    "either_girl=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(10000):\n",
    "    younger = random_kid()\n",
    "    older = random_kid()\n",
    "    if older == 'girl' and younger == 'girl':\n",
    "        both_girls = both_girls + 1\n",
    "    if older == 'girl':\n",
    "        older_girl = older_girl + 1\n",
    "    if older == 'girl' or younger == 'girl':\n",
    "        either_girl = either_girl + 1        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "both_girls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "older_girl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "either_girl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uniform_pdf(x):\n",
    "    return 1 if x>=0 and x<1 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uniform_cdf(x):\n",
    "    if x<0: return 0\n",
    "    if x<1: return x\n",
    "    else: return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normal_pdf(x, mu=0, sigma=1):\n",
    "    sqrt_two_pi = math.sqrt(2 * math.pi)\n",
    "    return (math.exp(-(x-mu) ** 2 / 2 / sigma ** 2) / (sqrt_two_pi * sigma))\n",
    "\n",
    "def plot_normal_pdfs(plt):\n",
    "    xs = [x / 10.0 for x in range(-50, 50)]\n",
    "    plt.plot(xs,[normal_pdf(x,sigma=1) for x in xs],'-',label='mu=0,sigma=1')\n",
    "    plt.plot(xs,[normal_pdf(x,sigma=2) for x in xs],'--',label='mu=0,sigma=2')\n",
    "    plt.plot(xs,[normal_pdf(x,sigma=0.5) for x in xs],':',label='mu=0,sigma=0.5')\n",
    "    plt.plot(xs,[normal_pdf(x,mu=-1)   for x in xs],'-.',label='mu=-1,sigma=1')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def normal_cdf(x, mu=0,sigma=1):\n",
    "    return (1 + math.erf((x - mu) / math.sqrt(2) / sigma)) / 2\n",
    "\n",
    "def plot_normal_cdfs(plt):\n",
    "    xs = [x / 10.0 for x in range(-50, 50)]\n",
    "    plt.plot(xs,[normal_cdf(x,sigma=1) for x in xs],'-',label='mu=0,sigma=1')\n",
    "    plt.plot(xs,[normal_cdf(x,sigma=2) for x in xs],'--',label='mu=0,sigma=2')\n",
    "    plt.plot(xs,[normal_cdf(x,sigma=0.5) for x in xs],':',label='mu=0,sigma=0.5')\n",
    "    plt.plot(xs,[normal_cdf(x,mu=-1) for x in xs],'-.',label='mu=-1,sigma=1')\n",
    "    plt.legend(loc=4) # снизу справа\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_normal_pdfs(plt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverse_normal_cdf(p, mu=0, sigma=1, tolerance=0.00001):\n",
    "    \"\"\"найти приближенную инверсию, используя двоичный поиск\"\"\"\n",
    "\n",
    "    # если не стандартизировано, то стандартизировать и нормализовать\n",
    "    if mu != 0 or sigma != 1:\n",
    "        return mu + sigma * inverse_normal_cdf(p, tolerance=tolerance)\n",
    "\n",
    "    low_z, low_p = -10.0, 0            # normal_cdf(-10) = (очень близко к) 0\n",
    "    hi_z,  hi_p  =  10.0, 1            # normal_cdf(10) = (очень близко к) 1\n",
    "    while hi_z - low_z > tolerance:\n",
    "        mid_z = (low_z + hi_z) / 2     # взять середину\n",
    "        mid_p = normal_cdf(mid_z)      # и значение ИФР в этом месте\n",
    "        if mid_p < p:\n",
    "            # значение середины все еще слишком низкое, искать выше его\n",
    "            low_z, low_p = mid_z, mid_p\n",
    "        elif mid_p > p:\n",
    "            # значение середины все еще слишком высокое, искать ниже\n",
    "            hi_z, hi_p = mid_z, mid_p\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    return mid_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inverse_normal_cdf(0.95, mu=0, sigma=1, tolerance=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bernoulli_trial(p):\n",
    "    return 1 if random.random() < p else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binomial(n,p):\n",
    "    return sum(bernoulli_trial(p) for _ in range(n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binomial(p=0.75, n=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_hist(p, n, num_points):\n",
    "\n",
    "    data = [binomial(n, p) for _ in range(num_points)]\n",
    "\n",
    "    # столбчатая диаграмма, показывающая фактические биномиальные выборки\n",
    "    histogram = Counter(data)\n",
    "    plt.bar([x - 0.4 for x in histogram.keys()],\n",
    "            [v / num_points for v in histogram.values()],\n",
    "            0.8,\n",
    "            color='0.75')\n",
    "\n",
    "    mu = p * n\n",
    "    sigma = math.sqrt(n * p * (1 - p))\n",
    "\n",
    "    # линейный график, показывающий нормальное приближение\n",
    "    xs = range(min(data), max(data) + 1)\n",
    "    ys = [normal_cdf(i + 0.5, mu, sigma) - normal_cdf(i - 0.5, mu, sigma)\n",
    "          for i in xs]\n",
    "    plt.plot(xs,ys)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_hist(0.95, 100, num_points=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_hist(0.50, 100, num_points=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_cdf(1, mu=0,sigma=1)-normal_cdf(-1, mu=0,sigma=1)# для себя, проверка нормального распределения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter([binomial(100, 0.95) for _ in range(100)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ГИПОТЕЗА И ВЫВОД"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normal_approximation_to_binomial(n,p):\n",
    "    mu = p * n\n",
    "    sigma = math.sqrt(p * (1 - p) * n)\n",
    "    return mu, sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binomial(100,0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_approximation_to_binomial(100,0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_probability_below = normal_cdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normal_probability_above(lo, mu=0, sigma=1):\n",
    "    return 1 - normal_cdf(lo, mu, sigma)\n",
    "def normal_probability_between(lo, hi, mu=0, sigma=1 ):\n",
    "    return normal_cdf(hi, mu, sigma) - normal_cdf(lo, mu, sigma)\n",
    "def normal_probability_outside(lo, hi, mu=0, sigma=1 ):\n",
    "    return 1 - normal_probability_between(lo, hi, mu, sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normal_upper_bound(probability, mu=0, sigma=1):\n",
    "    return inverse_normal_cdf(probability, mu, sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_upper_bound(0.95, mu=100, sigma=10) #с вероятностью 95% ниже этого верхнего предела"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normal_lower_bound(probability, mu=0, sigma=1):\n",
    "    return inverse_normal_cdf(1 - probability, mu, sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_lower_bound(0.95, mu=100, sigma=10) #с вероятностью 95% выше этого нижнего предела"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normal_two_sided_bounds(probability, mu=0, sigma=1):\n",
    "    tail_probability = (1 - probability) / 2\n",
    "    upper_bound = normal_lower_bound(tail_probability, mu, sigma)\n",
    "    lower_bound = normal_upper_bound(tail_probability, mu, sigma)\n",
    "    return lower_bound, upper_bound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_0, sigma_0 = normal_approximation_to_binomial(1000,0.5)\n",
    "print(mu_0, sigma_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lo, hi = normal_two_sided_bounds(0.95, mu_0, sigma_0)\n",
    "print(lo, hi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_1, sigma_1 = normal_approximation_to_binomial(1000,0.55)\n",
    "print(mu_1, sigma_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type_2_probability = normal_probability_between(lo, hi, mu_1, sigma_1)\n",
    "type_2_probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "power = 1 - type_2_probability\n",
    "power"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hi = normal_upper_bound(0.95, mu_0, sigma_0)\n",
    "hi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type_2_probability = normal_probability_below(hi, mu_1, sigma_1)\n",
    "type_2_probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "power = 1 - type_2_probability\n",
    "power"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def two_sided_p_value(x, mu=0, sigma=1):\n",
    "    if x>=mu:\n",
    "        return 2 * normal_probability_above(x, mu, sigma)\n",
    "    else:\n",
    "        return 2 * normal_probability_below(x, mu, sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "two_sided_p_value(529.5, mu_0, sigma_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_probability_above(529.5, mu_0, sigma_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_extreme_values():\n",
    "    extreme_value_count = 0\n",
    "    for _ in range(10000):\n",
    "        num_heads = sum(1 if random.random() < 0.5 else 0    # подсчитать число орлов\n",
    "                        for _ in range(1000))                # при 1000 бросках\n",
    "        if num_heads >= 530 or num_heads <= 470:             # подсчитать, как часто\n",
    "            extreme_value_count += 1                         # число - 'предельно'\n",
    "\n",
    "    return extreme_value_count / 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "count_extreme_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# проведение A/B-тестирования\n",
    "#\n",
    "##\n",
    "\n",
    "def estimated_parameters(N, n):\n",
    "    p = n / N\n",
    "    sigma = math.sqrt(p * (1 - p) / N)\n",
    "    return p, sigma\n",
    "\n",
    "def a_b_test_statistic(N_A, n_A, N_B, n_B):\n",
    "    p_A, sigma_A = estimated_parameters(N_A, n_A)\n",
    "    p_B, sigma_B = estimated_parameters(N_B, n_B)\n",
    "    return (p_B - p_A) / math.sqrt(sigma_A ** 2 + sigma_B ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z=a_b_test_statistic(1000, 200, 1000, 180)\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimated_parameters(1000, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimated_parameters(1000, 180)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "two_sided_p_value(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ГРАДИЕНТНЫЙ СПУСК"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def square(x):\n",
    "    return [i*i for i in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def difference_quotient(f,x,h):\n",
    "    return (f(x+h) - f(x)) / h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partial_difference_quotient(f,v,i,h):\n",
    "    w = [v_j + (h if j==i else 0) for j, v_j in enumerate(v)]\n",
    "    return (f(w) - f(v)) / h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_gradient (f,v,h=0.00001):\n",
    "    return [partial_difference_quotient(f,v,i,h) for i, _ in enumerate(v)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step(v, direction, step_size):\n",
    "    return [v_i + step_size * direction_i for v_i, direction_i in zip(v, direction)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_of_squares_gradient(v):\n",
    "    return [2*v_i for v_i in v]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = [random.randint(-10,10) for i in range(3)]\n",
    "tolerance = 0.0000001\n",
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    gradient = sum_of_squares_gradient(v)\n",
    "    next_v = step(v, gradient, -0.01)\n",
    "    if distance(next_v, v) < tolerance:\n",
    "        break\n",
    "    v = next_v\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe(f):\n",
    "    def safe_f(*args, **kwargs):\n",
    "        try:\n",
    "            return f(*args, **kwargs)\n",
    "        except:\n",
    "            return float('inf')\n",
    "        return safe_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minimize_batch(target_fn, gradient_fn, theta_0, tolerance=0.000001):\n",
    "    step_sizes = [100, 10, 1, 0.1, 0.01, 0.001, 0.0001, 0.00001]\n",
    "    theta = theta_0\n",
    "    target_fn = safe(target_fn)\n",
    "    value = target_fn(theta)\n",
    "    while True:\n",
    "        gradient = gradient_fn(theta)\n",
    "        next_thetas = [step(theta, gradient, -step_size) for step_size in step_sizes]\n",
    "        next_theta = min(next_thetas, key=target_fn)\n",
    "        next_value = target_fn(next_theta)\n",
    "        if abs(value - next_value) < tolerance:\n",
    "            return theta\n",
    "        else:\n",
    "            theta, value = next_theta, next_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def negate(f):\n",
    "    \"\"\"вернуть функцию, которая для любого входящего x возвращает -f(x)\"\"\"\n",
    "    return lambda *args, **kwargs: -f(*args, **kwargs)\n",
    "\n",
    "def negate_all(f):\n",
    "    \"\"\"то же самое, когда f возвращает список чисел\"\"\"\n",
    "    return lambda *args, **kwargs: [-y for y in f(*args, **kwargs)]\n",
    "\n",
    "def maximize_batch(target_fn, gradient_fn, theta_0, tolerance=0.000001):\n",
    "    return minimize_batch(negate(target_fn),\n",
    "                          negate_all(gradient_fn),\n",
    "                          theta_0,\n",
    "                          tolerance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def in_random_order(data):\n",
    "    \"\"\"генератор, который возвращает элементы данных в случайном порядке\"\"\"\n",
    "    indexes = [i for i, _ in enumerate(data)]  # создать список индексов\n",
    "    random.shuffle(indexes)                    # перемешать данные и\n",
    "    for i in indexes:                          # вернуть в этом порядке\n",
    "        yield data[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minimize_stochastic(target_fn, gradient_fn, x, y, theta_0, alpha_0=0.01):\n",
    "\n",
    "    data = list(zip(x, y))\n",
    "    theta = theta_0                             # первоначальная гипотеза\n",
    "    alpha = alpha_0                             # первоначальный размер шага\n",
    "    min_theta, min_value = None, float(\"inf\")   # минимум на этот момент\n",
    "    iterations_with_no_improvement = 0\n",
    "\n",
    "    # остановиться, если достигли 100 итераций без улучшений\n",
    "    while iterations_with_no_improvement < 100:\n",
    "        value = sum( target_fn(x_i, y_i, theta) for x_i, y_i in data )\n",
    "\n",
    "        if value < min_value:\n",
    "            # если найден новый минимум, то запомнить его\n",
    "            # и вернуться к первоначальному размеру шага\n",
    "            min_theta, min_value = theta, value\n",
    "            iterations_with_no_improvement = 0\n",
    "            alpha = alpha_0\n",
    "        else:\n",
    "            # в противном случае улучшений нет,\n",
    "            # поэтому пытаемся сжать размер шага\n",
    "            iterations_with_no_improvement += 1\n",
    "            alpha *= 0.9\n",
    "\n",
    "        # и делаем шаг градиента для каждой из точек данных\n",
    "        for x_i, y_i in in_random_order(data):\n",
    "            gradient_i = gradient_fn(x_i, y_i, theta)\n",
    "            theta = vector_subtract(theta, scalar_multiply(alpha, gradient_i))\n",
    "\n",
    "    return min_theta\n",
    "\n",
    "def maximize_stochastic(target_fn, gradient_fn, x, y, theta_0, alpha_0=0.01):\n",
    "    return minimize_stochastic(negate(target_fn),\n",
    "                               negate_all(gradient_fn),\n",
    "                               x, y, theta_0, alpha_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ОБРАБОТКА ДАННЫХ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# working_with_data.py\n",
    "\n",
    "\n",
    "\n",
    "def bucketize(point, bucket_size):\n",
    "    \"\"\"округлить точку до следующего наименьшего кратного\n",
    "    размера интервала bucket_size\"\"\"\n",
    "    return bucket_size * math.floor(point / bucket_size)\n",
    "\n",
    "def make_histogram(points, bucket_size):\n",
    "    \"\"\"сгруппировать точки и подсчитать количество в интервале\"\"\"\n",
    "    return Counter(bucketize(point, bucket_size) for point in points)\n",
    "\n",
    "def plot_histogram(points, bucket_size, title=\"\"):\n",
    "    histogram = make_histogram(points, bucket_size)\n",
    "    plt.bar(histogram.keys(), histogram.values(), width=bucket_size)\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "def compare_two_distributions():\n",
    "\n",
    "    random.seed(0)\n",
    "\n",
    "    uniform = [random.randrange(-100,101) for _ in range(200)]\n",
    "    normal = [57 * inverse_normal_cdf(random.random())\n",
    "              for _ in range(200)]\n",
    "\n",
    "    plot_histogram(uniform, 10, \"Гистограмма равномерного распределения\")\n",
    "    plot_histogram(normal,  10, \"Гистограмма нормального распределения\")\n",
    "\n",
    "def random_normal():\n",
    "    \"\"\"возвращает случайную выборку из стандартного\n",
    "    нормального распределения\"\"\"\n",
    "    return inverse_normal_cdf(random.random())\n",
    "\n",
    "xs = [random_normal() for _ in range(1000)]\n",
    "ys1 = [ x + random_normal() / 2 for x in xs]\n",
    "ys2 = [-x + random_normal() / 2 for x in xs]\n",
    "\n",
    "\n",
    "def scatter():\n",
    "    plt.scatter(xs, ys1, marker='.', color='black', label='ys1')\n",
    "    plt.scatter(xs, ys2, marker='.', color='gray',  label='ys2')\n",
    "    plt.xlabel('xs')\n",
    "    plt.ylabel('ys')\n",
    "    plt.legend(loc=9)\n",
    "    plt.show()\n",
    "\n",
    "def correlation_matrix(data):\n",
    "    \"\"\"возвращает матрицу num_columns x num_columns, где запись\n",
    "    в ячейке (i, j) - это корреляция между столбцами i и j данных\"\"\"\n",
    "\n",
    "    _, num_columns = shape(data)\n",
    "\n",
    "    def matrix_entry(i, j):\n",
    "        return correlation(get_column(data, i), get_column(data, j))\n",
    "\n",
    "    return make_matrix(num_columns, num_columns, matrix_entry)\n",
    "\n",
    "def make_scatterplot_matrix():\n",
    "\n",
    "    # сначала сгенерировать немного случайных данных\n",
    "\n",
    "    num_points = 100\n",
    "\n",
    "    def random_row():\n",
    "        row = [None, None, None, None]\n",
    "        row[0] = random_normal()\n",
    "        row[1] = -5 * row[0] + random_normal()\n",
    "        row[2] = row[0] + row[1] + 5 * random_normal()\n",
    "        row[3] = 6 if row[2] > -2 else 0\n",
    "        return row\n",
    "    random.seed(0)\n",
    "    data = [random_row()\n",
    "            for _ in range(num_points)]\n",
    "\n",
    "    # затем отобразить на графике\n",
    "\n",
    "    _, num_columns = shape(data)\n",
    "    fig, ax = plt.subplots(num_columns, num_columns)\n",
    "\n",
    "    for i in range(num_columns):\n",
    "        for j in range(num_columns):\n",
    "\n",
    "            # разбросать столбец j по оси X напротив столбца i на оси Y\n",
    "            if i != j: ax[i][j].scatter(get_column(data, j), get_column(data, i))\n",
    "\n",
    "            # если не i == j, показать имя серии\n",
    "            else: ax[i][j].annotate(\"series \" + str(i), (0.5, 0.5),\n",
    "                                    xycoords='axes fraction',\n",
    "                                    ha=\"center\", va=\"center\")\n",
    "\n",
    "            # затем спрятать осевые метки за исключением левой и нижней диаграмм\n",
    "            if i < num_columns - 1: ax[i][j].xaxis.set_visible(False)\n",
    "            if j > 0: ax[i][j].yaxis.set_visible(False)\n",
    "\n",
    "    # настроить правую нижнюю и левую верхнюю осевые метки,\n",
    "    # которые некорректны, потому что на их диаграммах выводится только текст\n",
    "    ax[-1][-1].set_xlim(ax[0][-1].get_xlim())\n",
    "    ax[0][0].set_ylim(ax[0][1].get_ylim())\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def parse_row(input_row, parsers):\n",
    "    \"\"\"применить соответствующий анализатор из заданного списка (некоторые\n",
    "    могут быть пустыми (None)) к каждому элементу input_row\"\"\"\n",
    "    return [parser(value) if parser is not None else value\n",
    "            for value, parser in zip(input_row, parsers)]\n",
    "\n",
    "def parse_rows_with(reader, parsers):\n",
    "    \"\"\"обертывает объект reader, применяя анализаторы к каждой строке\"\"\"\n",
    "    for row in reader:\n",
    "        yield parse_row(row, parsers)\n",
    "\n",
    "def try_or_none(f):\n",
    "    \"\"\"обертывает функцию f, возвращая None, если f вызывает исключение;\n",
    "    подразумевается, что f - функция одного входящего аргумента\"\"\"\n",
    "    def f_or_none(x):\n",
    "        try: return f(x)\n",
    "        except: return None\n",
    "    return f_or_none\n",
    "\n",
    "def parse_row(input_row, parsers):\n",
    "    return [try_or_none(parser)(value) if parser is not None else value\n",
    "            for value, parser in zip(input_row, parsers)]\n",
    "\n",
    "def try_parse_field(field_name, value, parser_dict):\n",
    "    \"\"\"попытаться преобразовать значение при помощи соответствующей\n",
    "    функции из словаря parser_dict с анализаторами\"\"\"\n",
    "    parser = parser_dict.get(field_name) # None, если нет такой записи\n",
    "    if parser is not None:\n",
    "        return try_or_none(parser)(value)\n",
    "    else:\n",
    "        return value\n",
    "\n",
    "def parse_dict(input_dict, parser_dict):\n",
    "    return { field_name : try_parse_field(field_name, value, parser_dict)\n",
    "             for field_name, value in input_dict.items() }\n",
    "\n",
    "#\n",
    "#\n",
    "# УПРАВЛЕНИЕ ДАННЫМИ \n",
    "#\n",
    "#\n",
    "\n",
    "def picker(field_name):\n",
    "    \"\"\"возвращает функцию, которая извлекает поле из dict\"\"\"\n",
    "    return lambda row: row[field_name]\n",
    "\n",
    "def pluck(field_name, rows):\n",
    "    \"\"\"преобразует список словарей dict в список значений\n",
    "    согласно field_name\"\"\"\n",
    "    return map(picker(field_name), rows)\n",
    "\n",
    "def group_by(grouper, rows, value_transform=None):\n",
    "    # ключ - это результат вычисления grouper, значение - это список строк\n",
    "    grouped = defaultdict(list)\n",
    "    for row in rows:\n",
    "        grouped[grouper(row)].append(row)\n",
    "    if value_transform is None:\n",
    "        return grouped\n",
    "    else:\n",
    "        return { key : value_transform(rows)\n",
    "                 for key, rows in grouped.items() }\n",
    "\n",
    "def percent_price_change(yesterday, today):\n",
    "    return today[\"closing_price\"] / yesterday[\"closing_price\"] - 1\n",
    "\n",
    "def day_over_day_changes(grouped_rows):\n",
    "    # сортировать строки по дате\n",
    "    ordered = sorted(grouped_rows, key=picker(\"date\"))\n",
    "    # объединить со смещением, чтобы получить последовательные пары\n",
    "    return [{ \"symbol\" : today[\"symbol\"],\n",
    "              \"date\" : today[\"date\"],\n",
    "              \"change\" : percent_price_change(yesterday, today) }\n",
    "             for yesterday, today in zip(ordered, ordered[1:])]\n",
    "\n",
    "#\n",
    "#\n",
    "# МНОГОМЕРНОЕ ШКАЛИРОВАНИЕ\n",
    "#\n",
    "#\n",
    "\n",
    "def scale(data_matrix):\n",
    "    \"\"\"вернуть средние и стандартные отклонения для каждого столбца\"\"\"\n",
    "    num_rows, num_cols = shape(data_matrix)\n",
    "    means = [mean(get_column(data_matrix,j))\n",
    "             for j in range(num_cols)]\n",
    "    stdevs = [standard_deviation(get_column(data_matrix,j))\n",
    "              for j in range(num_cols)]\n",
    "    return means, stdevs\n",
    "\n",
    "def rescale(data_matrix):\n",
    "    \"\"\"прошкалировать входящие данные так, чтобы каждый столбец\n",
    "    имел нулевое среднее значение и стандартное отклонение, равное 1;\n",
    "    пропускает столбцы без отклонения\"\"\"\n",
    "    means, stdevs = scale(data_matrix)\n",
    "\n",
    "    def rescaled(i, j):\n",
    "        if stdevs[j] > 0:\n",
    "            return (data_matrix[i][j] - means[j]) / stdevs[j]\n",
    "        else:\n",
    "            return data_matrix[i][j]\n",
    "\n",
    "    num_rows, num_cols = shape(data_matrix)\n",
    "    return make_matrix(num_rows, num_cols, rescaled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def de_mean_matrix(A):\n",
    "    nr, nc = shape(A)\n",
    "    column_means, _ = scale(A)\n",
    "    return make_matrix(nr, nc, lambda i, j: A[i][j] - column_means[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def direction(w):\n",
    "    mag = magnitude(w)\n",
    "    return [w_i / mag for w_i in w]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def directional_variance_i(x_i,w):\n",
    "    return dot(x_i, direction(w))**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def directional_variance(X,w):\n",
    "    return sum(directional_variance_i(x_i,w) for x_i in X)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def directional_variance_gradient_i(x_i,w):\n",
    "    projection_length = dot(x_i, direction(w))\n",
    "    return [2 * projection_length * x_ij for x_ij in x_i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def directional_variance_gradient(X, w):\n",
    "    return vector_sum(directional_variance_gradient_i(x_i,w) for x_i in X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_principal_component(X):\n",
    "    guess = [1 for _ in X[0]]\n",
    "    unscaled_maximizer = maximize_batch(\n",
    "        partial(directional_variance, X),           # is now a function of w\n",
    "        partial(directional_variance_gradient, X),  # is now a function of w\n",
    "        guess)\n",
    "    return direction(unscaled_maximizer)\n",
    "\n",
    "def first_principal_component_sgd(X):\n",
    "    guess = [1 for _ in X[0]]\n",
    "    unscaled_maximizer = maximize_stochastic(\n",
    "        lambda x, _, w: directional_variance_i(x, w),\n",
    "        lambda x, _, w: directional_variance_gradient_i(x, w),\n",
    "        X, [None for _ in X], guess)\n",
    "    return direction(unscaled_maximizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def project(v, w):\n",
    "    \"\"\"вернуть проекцию v на направление w\"\"\"\n",
    "    coefficient = dot(v, w)\n",
    "    return scalar_multiply(coefficient, w)\n",
    "\n",
    "def remove_projection_from_vector(v, w):\n",
    "    \"\"\"проецирует v в w и вычитает результат из v\"\"\"\n",
    "    return vector_subtract(v, project(v, w))\n",
    "\n",
    "def remove_projection(X, w):\n",
    "    \"\"\"для каждой строки X\n",
    "    проецирует строку в w, и вычитает результат из строки\"\"\"\n",
    "    return [remove_projection_from_vector(x_i, w) for x_i in X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def principal_component_analysis(X, num_components):\n",
    "    components = []\n",
    "    for _ in range(num_components):\n",
    "        component = first_principal_component(X)\n",
    "        components.append(component)\n",
    "        X = remove_projection(X, component)\n",
    "\n",
    "    return components\n",
    "\n",
    "def transform_vector(v, components):\n",
    "    return [dot(v, w) for w in components]\n",
    "\n",
    "def transform(X, components):\n",
    "    return [transform_vector(x_i, components) for x_i in X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=[[1,2,4],[2,3,4],[6,7,8],[3,5,4],[7,8,9],[3,5,7],[1,4,7],[3,2,6],[5,6,7]]\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1=principal_component_analysis(X, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2=transform(X, X1)\n",
    "X2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# МАШИННОЕ ОБУЧЕНИЕ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(data, prob):\n",
    "    results = [], []\n",
    "    for row in data:\n",
    "        results[0 if random.random() < prob else 1].append(row)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(x,y,test_pct):\n",
    "    data = zip(x,y)\n",
    "    train, test = split_data(data, 1-test_pct)\n",
    "    x_train, y_train = zip(*train)\n",
    "    x_test, y_test = zip(*test)\n",
    "    return x_train, y_train, x_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K БЛИЖАЙШИХ СОСЕДЕЙ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def raw_majority_vote(labels):\n",
    "    votes = Counter(labels)\n",
    "    winner, _ = votes.most_common(1)[0]\n",
    "    return winner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels=[1,1,1,0,2,3,4,5,5,5,5]\n",
    "raw_majority_vote(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(labels).most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def majority_vote(labels):\n",
    "    vote_counts = Counter(labels)\n",
    "    winner, winner_count = vote_counts.most_common(1)[0]\n",
    "    num_winners = len([count for count in vote_counts.values() if count==winner_count])\n",
    "    if num_winners==1:\n",
    "        return winner\n",
    "    else:\n",
    "        return majority_vote(labels[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn_classify(k, labeled_points, new_point):\n",
    "    \"\"\"каждая маркированная точка должна быть представлена\n",
    "    парой (точка, метка)\"\"\"\n",
    "\n",
    "    # упорядочить маркированные точки от ближайшей до самой удаленной\n",
    "    by_distance = sorted(labeled_points,\n",
    "                         key=lambda point_label: distance(point_label[0], new_point))\n",
    "\n",
    "    # найти метки для k ближайших\n",
    "    k_nearest_labels = [label for _, label in by_distance[:k]]\n",
    "\n",
    "    # и дать им проголосовать\n",
    "    return majority_vote(k_nearest_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_classify(3, cities, [-109, 31])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "by_distance=sorted(cities, key=lambda point_label: distance(point_label[0], [-109, 31]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_nearest_labels = [label for _, label in by_distance[:5]]\n",
    "k_nearest_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# НАИВНЫЙ БАЙЕС"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(message):\n",
    "    message = message.lower()                       # преобразовать с строчные\n",
    "    all_words = re.findall(\"[a-z0-9']+\", message)   # извлечь слова\n",
    "    return set(all_words)                           # удалить повторы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "message=\"from lib.machine_learning import split_data\"\n",
    "type(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words(training_set):\n",
    "    \"\"\"обучающая выборка состоит из пар (сообщение, спам?\"\"\"\n",
    "    counts = defaultdict(lambda: [0, 0])\n",
    "    for message, is_spam in training_set:\n",
    "        for word in tokenize(message):\n",
    "            counts[word][0 if is_spam else 1] += 1\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_probabilities(counts, total_spams, total_non_spams, k=0.5):\n",
    "    \"\"\"преобразовать частотности word_counts в список триплетов\n",
    "    слово w, p(w | spam) и p(w | ~spam)\"\"\"\n",
    "    return [(w,\n",
    "             (spam + k) / (total_spams + 2 * k),\n",
    "             (non_spam + k) / (total_non_spams + 2 * k))\n",
    "             for w, (spam, non_spam) in counts.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = [('from collections import Counter, defaultdict', 1), ('from lib.machine_learning import split_data',0), ('import math, random, re, glob',0), ('def word_probabilities(counts, total_spams, total_non_spams, k=0.5)',1), ('def spam_probability(word_probs, message)',0), ('return prob_if_spam / (prob_if_spam + prob_if_not_spam)',1), ('def spam_probability(word_probs, message)',0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_words(training_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_probs=word_probabilities(count_words(training_set), 3, 4, k=0.5)\n",
    "word_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spam_probability(word_probs, message):\n",
    "    message_words = tokenize(message)\n",
    "    log_prob_if_spam = log_prob_if_not_spam = 0.0\n",
    "\n",
    "    for word, prob_if_spam, prob_if_not_spam in word_probs:\n",
    "\n",
    "        # если СЛОВО в сообщении появляется, то\n",
    "        # добавить лог-вероятность встретить его в сообщении\n",
    "        if word in message_words:\n",
    "            log_prob_if_spam += math.log(prob_if_spam)\n",
    "            log_prob_if_not_spam += math.log(prob_if_not_spam)\n",
    "\n",
    "        # если СЛОВО в сообщении не появляется, то\n",
    "        # добавить лог-вероятность НЕ встретить его в сообщении,\n",
    "        # вычисляемое как log(1 - вероятность встретить его в сообщении)\n",
    "        else:\n",
    "            log_prob_if_spam += math.log(1.0 - prob_if_spam)\n",
    "            log_prob_if_not_spam += math.log(1.0 - prob_if_not_spam)\n",
    "\n",
    "    prob_if_spam = math.exp(log_prob_if_spam)\n",
    "    prob_if_not_spam = math.exp(log_prob_if_not_spam)\n",
    "    return prob_if_spam / (prob_if_spam + prob_if_not_spam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_probability(word_probs, 'knock knock on heavens door glob')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayesClassifier:\n",
    "\n",
    "    def __init__(self, k=0.5):\n",
    "        self.k = k\n",
    "        self.word_probs = []\n",
    "\n",
    "    def train(self, training_set):\n",
    "\n",
    "        # подсчитать спамные и неспамные сообщения\n",
    "        num_spams = len([is_spam\n",
    "                         for message, is_spam in training_set\n",
    "                         if is_spam])\n",
    "        num_non_spams = len(training_set) - num_spams\n",
    "\n",
    "        # пропустить обучающую выборку через \"конвейер\"\n",
    "        word_counts = count_words(training_set)\n",
    "        self.word_probs = word_probabilities(word_counts,\n",
    "                                             num_spams,\n",
    "                                             num_non_spams,\n",
    "                                             self.k)\n",
    "\n",
    "    def classify(self, message):\n",
    "        return spam_probability(self.word_probs, message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_subject_data(path):\n",
    "\n",
    "    data = []\n",
    "\n",
    "    # регулярное выражение для удаления начального слова \"Subject:\" \n",
    "    # и любых пробельных символов после него\n",
    "    subject_regex = re.compile(r\"^Subject:\\s+\")\n",
    "\n",
    "    # glob.glob возвращает имена файлов,\n",
    "    # соответствующие шаблону поиска по указанному пути    \n",
    "    for fn in glob.glob(path):\n",
    "        is_spam = \"ham\" not in fn\n",
    "\n",
    "        with open(fn,'r',encoding='ISO-8859-1') as file:\n",
    "            for line in file:\n",
    "                if line.startswith(\"Subject:\"):\n",
    "                    subject = subject_regex.sub(\"\", line).strip()\n",
    "                    data.append((subject, is_spam))\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path=r'C:/Users/user/Documents/book_files joel grus/FTP/code-python3-ru/data/spam/*/*'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=get_subject_data(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(0)\n",
    "train_data, test_data = split_data(data, 0.75)\n",
    "classifier = NaiveBayesClassifier()\n",
    "classifier.train(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.word_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classified = [(subject, is_spam, classifier.classify(subject))\n",
    "              for subject, is_spam in test_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = Counter((is_spam, spam_probability > 0.5) # (фактическая, прогнозная)\n",
    "                     for _, is_spam, spam_probability in classified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ПРОСТАЯ ЛИНЕЙНАЯ РЕГРЕССИЯ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(alpha, beta, x_i):\n",
    "    return beta * x_i + alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error(alpha, beta, x_i, y_i):\n",
    "    return y_i - predict(alpha, beta, x_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_of_squared_errors(alpha, beta, x, y):\n",
    "    return sum(error(alpha, beta, x_i, y_i) ** 2\n",
    "               for x_i, y_i in zip(x, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def least_squares_fit(x,y):\n",
    "    \"\"\"при заданных обучающих значениях x и y,\n",
    "    найти значения alpha и beta на основе МНК\"\"\"\n",
    "    beta = correlation(x, y) * standard_deviation(y) / standard_deviation(x)\n",
    "    alpha = mean(y) - beta * mean(x)\n",
    "    return alpha, beta\n",
    "\n",
    "def total_sum_of_squares(y):\n",
    "    \"\"\"полная сумма квадратов отклонений y_i от их среднего\"\"\"\n",
    "    return sum(v ** 2 for v in de_mean(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_sum_of_squares(y):\n",
    "    \"\"\"полная сумма квадратов отклонений y_i от их среднего\"\"\"\n",
    "    return sum(v ** 2 for v in de_mean(y))\n",
    "\n",
    "def r_squared(alpha, beta, x, y):\n",
    "    \"\"\"доля вариации в y, объясненная моделью, равна\n",
    "    1 - доля вариации в y, не объясненная моделью\"\"\"\n",
    "\n",
    "    return 1.0 - (sum_of_squared_errors(alpha, beta, x, y) /\n",
    "                  total_sum_of_squares(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def squared_error(x_i, y_i, theta):\n",
    "    alpha, beta = theta\n",
    "    return error(alpha, beta, x_i, y_i) ** 2\n",
    "\n",
    "def squared_error_gradient(x_i, y_i, theta):\n",
    "    alpha, beta = theta\n",
    "    return [-2 * error(alpha, beta, x_i, y_i),       # частная производная alpha\n",
    "            -2 * error(alpha, beta, x_i, y_i) * x_i] # частная производная beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"градиентный спуск:\")\n",
    "# выбрать случайное значение для запуска\n",
    "random.seed(0)\n",
    "theta = [random.random(), random.random()]\n",
    "alpha, beta = minimize_stochastic(squared_error,\n",
    "                                      squared_error_gradient,\n",
    "                                      num_friends_good,\n",
    "                                      daily_minutes_good,\n",
    "                                      theta,\n",
    "                                      0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# МНОЖЕСТВЕННАЯ РЕГРЕССИЯ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta = [1,2,3,4]\n",
    "x_i = [6,7,8,9]\n",
    "def predict(x_i, beta):\n",
    "    return dot(x_i, beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(x_i, beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error(x_i, y_i, beta):\n",
    "    return y_i - predict(x_i, beta)\n",
    "def squared_error(x_i, y_i, beta):\n",
    "    return error(x_i, y_i, beta) ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def squared_error_gradient(x_i, y_i, beta):\n",
    "    \"\"\"градиент (относительно beta), соответствующий i-ой квадратичной ошибке\"\"\"\n",
    "    return [-2 * x_ij * error(x_i, y_i, beta)\n",
    "            for x_ij in x_i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_beta(x, y):\n",
    "    beta_initial = [random.random() for x_i in x[0]]\n",
    "    return minimize_stochastic(squared_error,\n",
    "                               squared_error_gradient,\n",
    "                               x, y,\n",
    "                               beta_initial,\n",
    "                               0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    x = [[1,49,4,0],[1,41,9,0],[1,40,8,0],[1,25,6,0],[1,21,1,0],\n",
    "         [1,21,0,0],[1,19,3,0],[1,19,0,0],[1,18,9,0],[1,18,8,0],\n",
    "         [1,16,4,0],[1,15,3,0],[1,15,0,0],[1,15,2,0],[1,15,7,0],\n",
    "         [1,14,0,0],[1,14,1,0],[1,13,1,0],[1,13,7,0],[1,13,4,0],\n",
    "         [1,13,2,0],[1,12,5,0],[1,12,0,0],[1,11,9,0],[1,10,9,0],\n",
    "         [1,10,1,0],[1,10,1,0],[1,10,7,0],[1,10,9,0],[1,10,1,0],\n",
    "         [1,10,6,0],[1,10,6,0],[1,10,8,0],[1,10,10,0],[1,10,6,0],\n",
    "         [1,10,0,0],[1,10,5,0],[1,10,3,0],[1,10,4,0],[1,9,9,0],\n",
    "         [1,9,9,0],[1,9,0,0],[1,9,0,0],[1,9,6,0],[1,9,10,0],\n",
    "         [1,9,8,0],[1,9,5,0],[1,9,2,0],[1,9,9,0],[1,9,10,0],\n",
    "         [1,9,7,0],[1,9,2,0],[1,9,0,0],[1,9,4,0],[1,9,6,0],\n",
    "         [1,9,4,0],[1,9,7,0],[1,8,3,0],[1,8,2,0],[1,8,4,0],\n",
    "         [1,8,9,0],[1,8,2,0],[1,8,3,0],[1,8,5,0],[1,8,8,0],\n",
    "         [1,8,0,0],[1,8,9,0],[1,8,10,0],[1,8,5,0],[1,8,5,0],\n",
    "         [1,7,5,0],[1,7,5,0],[1,7,0,0],[1,7,2,0],[1,7,8,0],\n",
    "         [1,7,10,0],[1,7,5,0],[1,7,3,0],[1,7,3,0],[1,7,6,0],\n",
    "         [1,7,7,0],[1,7,7,0],[1,7,9,0],[1,7,3,0],[1,7,8,0],\n",
    "         [1,6,4,0],[1,6,6,0],[1,6,4,0],[1,6,9,0],[1,6,0,0],\n",
    "         [1,6,1,0],[1,6,4,0],[1,6,1,0],[1,6,0,0],[1,6,7,0],\n",
    "         [1,6,0,0],[1,6,8,0],[1,6,4,0],[1,6,2,1],[1,6,1,1],\n",
    "         [1,6,3,1],[1,6,6,1],[1,6,4,1],[1,6,4,1],[1,6,1,1],\n",
    "         [1,6,3,1],[1,6,4,1],[1,5,1,1],[1,5,9,1],[1,5,4,1],\n",
    "         [1,5,6,1],[1,5,4,1],[1,5,4,1],[1,5,10,1],[1,5,5,1],\n",
    "         [1,5,2,1],[1,5,4,1],[1,5,4,1],[1,5,9,1],[1,5,3,1],\n",
    "         [1,5,10,1],[1,5,2,1],[1,5,2,1],[1,5,9,1],[1,4,8,1],\n",
    "         [1,4,6,1],[1,4,0,1],[1,4,10,1],[1,4,5,1],[1,4,10,1],\n",
    "         [1,4,9,1],[1,4,1,1],[1,4,4,1],[1,4,4,1],[1,4,0,1],\n",
    "         [1,4,3,1],[1,4,1,1],[1,4,3,1],[1,4,2,1],[1,4,4,1],\n",
    "         [1,4,4,1],[1,4,8,1],[1,4,2,1],[1,4,4,1],[1,3,2,1],\n",
    "         [1,3,6,1],[1,3,4,1],[1,3,7,1],[1,3,4,1],[1,3,1,1],\n",
    "         [1,3,10,1],[1,3,3,1],[1,3,4,1],[1,3,7,1],[1,3,5,1],\n",
    "         [1,3,6,1],[1,3,1,1],[1,3,6,1],[1,3,10,1],[1,3,2,1],\n",
    "         [1,3,4,1],[1,3,2,1],[1,3,1,1],[1,3,5,1],[1,2,4,1],\n",
    "         [1,2,2,1],[1,2,8,1],[1,2,3,1],[1,2,1,1],[1,2,9,1],\n",
    "         [1,2,10,1],[1,2,9,1],[1,2,4,1],[1,2,5,1],[1,2,0,1],\n",
    "         [1,2,9,1],[1,2,9,1],[1,2,0,1],[1,2,1,1],[1,2,1,1],\n",
    "         [1,2,4,1],[1,1,0,1],[1,1,2,1],[1,1,2,1],[1,1,5,1],\n",
    "         [1,1,3,1],[1,1,10,1],[1,1,6,1],[1,1,0,1],[1,1,8,1],\n",
    "         [1,1,6,1],[1,1,4,1],[1,1,9,1],[1,1,9,1],[1,1,4,1],\n",
    "         [1,1,2,1],[1,1,9,1],[1,1,0,1],[1,1,8,1],[1,1,6,1],\n",
    "         [1,1,1,1],[1,1,1,1],[1,1,5,1]]\n",
    "\n",
    "    daily_minutes_good = [68.77,51.25,52.08,38.36,44.54,57.13,51.4,41.42,31.22,34.76,\n",
    "                          54.01,38.79,47.59,49.1,27.66,41.03,36.73,48.65,28.12,46.62,\n",
    "                          35.57,32.98,35,26.07,23.77,39.73,40.57,31.65,31.21,36.32,\n",
    "                          20.45,21.93,26.02,27.34,23.49,46.94,30.5,33.8,24.23,21.4,\n",
    "                          27.94,32.24,40.57,25.07,19.42,22.39,18.42,46.96,23.72,26.41,\n",
    "                          26.97,36.76,40.32,35.02,29.47,30.2,31,38.11,38.18,36.31,21.03,\n",
    "                          30.86,36.07,28.66,29.08,37.28,15.28,24.17,22.31,30.17,25.53,\n",
    "                          19.85,35.37,44.6,17.23,13.47,26.33,35.02,32.09,24.81,19.33,\n",
    "                          28.77,24.26,31.98,25.73,24.86,16.28,34.51,15.23,39.72,40.8,\n",
    "                          26.06,35.76,34.76,16.13,44.04,18.03,19.65,32.62,35.59,39.43,\n",
    "                          14.18,35.24,40.13,41.82,35.45,36.07,43.67,24.61,20.9,21.9,\n",
    "                          18.79,27.61,27.21,26.61,29.77,20.59,27.53,13.82,33.2,25,33.1,\n",
    "                          36.65,18.63,14.87,22.2,36.81,25.53,24.62,26.25,18.21,28.08,\n",
    "                          19.42,29.79,32.8,35.99,28.32,27.79,35.88,29.06,36.28,14.1,\n",
    "                          36.63,37.49,26.9,18.58,38.48,24.48,18.95,33.55,14.24,29.04,\n",
    "                          32.51,25.63,22.22,19,32.73,15.16,13.9,27.2,32.01,29.27,33,\n",
    "                          13.74,20.42,27.32,18.23,35.35,28.48,9.08,24.62,20.12,35.26,\n",
    "                          19.92,31.02,16.49,12.16,30.7,31.22,34.65,13.13,27.51,33.2,\n",
    "                          31.57,14.1,33.42,17.44,10.12,24.42,9.82,23.39,30.93,15.03,\n",
    "                          21.67,31.09,33.29,22.61,26.89,23.48,8.38,27.81,32.35,23.84]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    random.seed(0)\n",
    "    beta = estimate_beta(x, daily_minutes_good) # [30.63, 0.972, -1.868, 0.911]\n",
    "    print(\"beta\", beta)\n",
    "    #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# продолжая тему множественной регрессии. на стр. 208 книги прекрасно подается концепция добавления/изменения переменных, то есть признаков"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiple_r_squared(x, y, beta):\n",
    "    sum_of_squared_errors = sum(error(x_i, y_i, beta) ** 2\n",
    "                                for x_i, y_i in zip(x, y))\n",
    "    return 1.0 - sum_of_squared_errors / total_sum_of_squares(y)\n",
    "print(\"r-квадрат\", multiple_r_squared(x, daily_minutes_good, beta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap_sample(data):\n",
    "    \"\"\"случайно отбирает len(data) элементов с возвратом (с повторами\"\"\"\n",
    "    return [random.choice(data) for _ in data]\n",
    "\n",
    "def bootstrap_statistic(data, stats_fn, num_samples):\n",
    "    \"\"\"оценивает функцию stats_fn на бустрап-выборках из данных\n",
    "    в количестве num_samples\"\"\"\n",
    "    return [stats_fn(bootstrap_sample(data))\n",
    "            for _ in range(num_samples)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # 101 точек, чьи значения очень близки к 100\n",
    "    close_to_100 = [99.5 + random.random() for _ in range(101)]\n",
    "\n",
    "    # 101 точек, из них 50 значений близки к 0, а другие 50 близки к 200\n",
    "    far_from_100 = ([99.5 + random.random()] +\n",
    "                    [random.random() for _ in range(50)] +\n",
    "                    [200 + random.random() for _ in range(50)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    print()\n",
    "    print(\"bootstrap_statistic(close_to_100, median, 100):\")\n",
    "    print(bootstrap_statistic(close_to_100, median, 100))\n",
    "    print()\n",
    "    print(\"bootstrap_statistic(far_from_100, median, 100):\")\n",
    "    print(bootstrap_statistic(far_from_100, median, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_sample_beta(sample):\n",
    "    x_sample, y_sample = list(zip(*sample)) # трюк с \"разъединением\" списка\n",
    "    return estimate_beta(x_sample, y_sample)\n",
    "\n",
    "def p_value(beta_hat_j, sigma_hat_j):\n",
    "    if beta_hat_j > 0:\n",
    "        return 2 * (1 - normal_cdf(beta_hat_j / sigma_hat_j))\n",
    "    else:\n",
    "        return 2 * normal_cdf(beta_hat_j / sigma_hat_j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    random.seed(0) # чтобы получить повторимые результаты\n",
    "\n",
    "    #bootstrap_betas = bootstrap_statistic(list(zip(x, daily_minutes_good)),\n",
    "   #                                       estimate_sample_beta,\n",
    "    #                                      100)\n",
    "\n",
    "    #bootstrap_standard_errors = [\n",
    "    #    standard_deviation([beta[i] for beta in bootstrap_betas])\n",
    "     #   for i in range(4)]\n",
    "\n",
    "    print()\n",
    "    #print(\"стандартные ошибки бутстрапированных выборок\", bootstrap_standard_errors)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# таким образом, приходим к понимаю того, что p-value для коэффициента признака является выразителем вероятности отклониться больше чем на стандартное отклонение от найденного значения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_penalty(beta, alpha):\n",
    "    return alpha * dot(beta[1:], beta[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def squared_error_ridge(x_i, y_i, beta, alpha):\n",
    "    \"\"\"оценить ошибку плюс штраф для beta\"\"\"\n",
    "    return error(x_i, y_i, beta) ** 2 + ridge_penalty(beta, alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_penalty_gradient(beta, alpha):\n",
    "    \"\"\"градиент штрафа в гребневой регрессии\"\"\"\n",
    "    return [0] + [2 * alpha * beta_j for beta_j in beta[1:]]\n",
    "\n",
    "def squared_error_ridge_gradient(x_i, y_i, beta, alpha):\n",
    "    \"\"\"градиент, соответствующий i-ой квадратичной ошибке,\n",
    "    включая гребневой штраф\"\"\"\n",
    "    return vector_add(squared_error_gradient(x_i, y_i, beta),\n",
    "                      ridge_penalty_gradient(beta, alpha))\n",
    "\n",
    "def estimate_beta_ridge(x, y, alpha):\n",
    "    \"\"\"применить градиентный спуск для подгонки гребневой регрессии\n",
    "    со штрафом alpha\"\"\"\n",
    "    beta_initial = [random.random() for x_i in x[0]]\n",
    "    return minimize_stochastic(partial(squared_error_ridge, alpha=alpha),\n",
    "                               partial(squared_error_ridge_gradient,\n",
    "                                       alpha=alpha),\n",
    "                               x, y,\n",
    "                               beta_initial,\n",
    "                               0.001)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# закрепляем понятия ридж-регрессии и лассо-регрессии, добавления квадратов коэффициентов и абсолютных значений коэффициентов к функции потерь"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ЛОГИСТИЧЕСКАЯ РЕГРЕССИЯ - УСТНО ПРОЧИТАЛ ИЗВЕСТНЫЕ КОНЦЕПЦИИ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ДЕРЕВЬЯ ПРИНЯТИЯ РЕШЕНИЙ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(class_probabilities):\n",
    "    return sum(-p*math.log(p,2) for p in class_probabilities if p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_probabilities(labels):\n",
    "    total_count = len(labels)\n",
    "    return [count/total_count for count in Counter(labels).values()]\n",
    "\n",
    "\n",
    "def data_entropy(labeled_data):\n",
    "    labels = [label for _, label in labeled_data]\n",
    "    probabilities = class_probabilities(labels)\n",
    "    return entropy(probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partition_entropy(subsets):\n",
    "    total_count = sum(len(subset) for subset in subsets)\n",
    "    return sum(data_entropy(subset) * len(subset) / total_count for subset in subsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [\n",
    "        ({'level':'Senior','lang':'Java','tweets':'no','phd':'no'},   False),\n",
    "        ({'level':'Senior','lang':'Java','tweets':'no','phd':'yes'},  False),\n",
    "        ({'level':'Mid','lang':'Python','tweets':'no','phd':'no'},     True),\n",
    "        ({'level':'Junior','lang':'Python','tweets':'no','phd':'no'},  True),\n",
    "        ({'level':'Junior','lang':'R','tweets':'yes','phd':'no'},      True),\n",
    "        ({'level':'Junior','lang':'R','tweets':'yes','phd':'yes'},    False),\n",
    "        ({'level':'Mid','lang':'R','tweets':'yes','phd':'yes'},        True),\n",
    "        ({'level':'Senior','lang':'Python','tweets':'no','phd':'no'}, False),\n",
    "        ({'level':'Senior','lang':'R','tweets':'yes','phd':'no'},      True),\n",
    "        ({'level':'Junior','lang':'Python','tweets':'yes','phd':'no'}, True),\n",
    "        ({'level':'Senior','lang':'Python','tweets':'yes','phd':'yes'},True),\n",
    "        ({'level':'Mid','lang':'Python','tweets':'no','phd':'yes'},    True),\n",
    "        ({'level':'Mid','lang':'Java','tweets':'yes','phd':'no'},      True),\n",
    "        ({'level':'Junior','lang':'Python','tweets':'no','phd':'yes'},False)\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partition_by(inputs, attribute):\n",
    "    groups = defaultdict(list)\n",
    "    for input_ in inputs:\n",
    "        key = input_[0][attribute]\n",
    "        groups[key].append(input_)\n",
    "    return groups    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(partition_by(inputs, 'tweets').values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partition_entropy_by(inputs, attribute):\n",
    "    partitions = partition_by(inputs, attribute)\n",
    "    return partition_entropy(partitions.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partition_entropy_by(inputs, 'tweets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in ['level','lang','tweets','phd']:\n",
    "        print(key, partition_entropy_by(inputs, key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "senior_inputs = [(input_, label) for input_, label in inputs if input_['level']=='Senior']\n",
    "for key in ['lang','tweets','phd']:\n",
    "        print(key, partition_entropy_by(senior_inputs, key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(tree, input):\n",
    "    if tree in [True, False]:\n",
    "        return tree\n",
    "    attribute, subtree_dict = tree\n",
    "    subtree_key = input.get(attribute)\n",
    "    \n",
    "    if subtree_key not in subtree_dict:\n",
    "        subtree_key = None\n",
    "    subtree = subtree_dict[subtree_key]\n",
    "    return classify(subtree, input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_tree_id3(inputs, split_candidates=None):\n",
    "    if split_candidates is None:\n",
    "        split_candidates = inputs[0][0].keys()\n",
    "    num_inputs = len(inputs)\n",
    "    num_trues = len([label for item, label in inputs if label])\n",
    "    num_falses = num_inputs - num_trues\n",
    "    \n",
    "    if num_trues == 0: return False\n",
    "    if num_falses == 0: return True\n",
    "    if not split_candidates:\n",
    "        return num_trues >= num_falses\n",
    "    \n",
    "    best_attribute = min(split_candidates, key=partial(partition_entropy_by, inputs))\n",
    "    partitions = partition_by(inputs, best_attribute)\n",
    "    new_candidates = [a for a in split_candidates if a != best_attribute]\n",
    "    subtrees = {atrribute_value: build_tree_id3(subset, new_candidates)\n",
    "                for atrribute_value, subset in partitions.items() }\n",
    "    subtrees[None] = num_trues > num_falses\n",
    "    \n",
    "    return (best_attribute, subtrees)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs[-1][0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = {'level':'Junior','lang':'Java','tweets':'yes','phd':'no'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input.get('tweets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = build_tree_id3(inputs)\n",
    "tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attribute, subtree_dict = tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subtree_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subtree_key = input.get(attribute)\n",
    "subtree_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subtree = subtree_dict[subtree_key]\n",
    "subtree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classify(tree, input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forest_classify(trees, input):\n",
    "    votes = [classify(tree, input) for tree in trees]\n",
    "    vote_counts = Counter(votes)\n",
    "    return vote_counts.most_common(1)[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# НЕЙРОННЫЕ СЕТИ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_function(x):\n",
    "    return 1 if x >= 0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perceptron_output(weights, bias, x):\n",
    "    calculation = dot(weights, x) + bias\n",
    "    return step_function(calculation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(t):\n",
    "    return 1 / (1 + math.exp(-t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neuron_output(weights, inputs):\n",
    "    return sigmoid(dot(weights, inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feed_forward(neural_network, input_vector):\n",
    "    \"\"\"принимает нейронную сеть (как список списков списков весов) и\n",
    "    вектор входящих сигналов;\n",
    "    возвращает результат прямого распространения входящих сигналов\"\"\"\n",
    "\n",
    "    outputs = []\n",
    "\n",
    "    for layer in neural_network:\n",
    "\n",
    "        input_with_bias = input_vector + [1]             # добавить величину смещения\n",
    "        output = [neuron_output(neuron, input_with_bias) # вычислить результат\n",
    "                  for neuron in layer]                   # для этого нейрона\n",
    "        outputs.append(output)                           # и запомнить его\n",
    "\n",
    "        # входом для следующего слоя становится\n",
    "        # вектор результатов текущего слоя\n",
    "        input_vector = output\n",
    "\n",
    "    return outputs\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neural_network = [[[20,20,-30], [20,20,-10]], [[-60,60,-30]]]\n",
    "input_vector = [0,0]\n",
    "feed_forward(neural_network, input_vector)[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backpropagate(network, input_vector, target):\n",
    "\n",
    "    hidden_outputs, outputs = feed_forward(network, input_vector)\n",
    "\n",
    "    # из производной сигмоидальной функции взято output * (1 - output)\n",
    "    output_deltas = [output * (1 - output) * (output - target[i])\n",
    "                     for i, output in enumerate(outputs)]\n",
    "\n",
    "    # понейронно скорректировать веса для слоя выходов (network[-1])\n",
    "    for i, output_neuron in enumerate(network[-1]):\n",
    "        for j, hidden_output in enumerate(hidden_outputs + [1]):\n",
    "            output_neuron[j] -= output_deltas[i] * hidden_output\n",
    "\n",
    "    # распространить ошибки на скрытый слой, двигаясь в обратную сторону\n",
    "    hidden_deltas = [hidden_output * (1 - hidden_output) *\n",
    "                      dot(output_deltas, [n[i] for n in network[-1]])\n",
    "                     for i, hidden_output in enumerate(hidden_outputs)]\n",
    "\n",
    "    # понейронно скорректировать веса для скрытого слоя (network[0])\n",
    "    for i, hidden_neuron in enumerate(network[0]):\n",
    "        for j, input in enumerate(input_vector + [1]):\n",
    "            hidden_neuron[j] -= hidden_deltas[i] * input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#network = [[[20,20,-30], [20,20,-10]], [[-60,60,-30]]]\n",
    "#input_vector = [0,0]\n",
    "hidden_outputs, outputs = feed_forward(network, input_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = [5]\n",
    "output_deltas = [output * (1 - output) * (output - target[i])\n",
    "                     for i, output in enumerate(outputs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_deltas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, output_neuron in enumerate(network[-1]):\n",
    "        for j, hidden_output in enumerate(hidden_outputs + [1]):\n",
    "            output_neuron[j] -= output_deltas[i] * hidden_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_neuron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_deltas = [hidden_output * (1 - hidden_output) *\n",
    "                      dot(output_deltas, [n[i] for n in network[-1]])\n",
    "                     for i, hidden_output in enumerate(hidden_outputs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_deltas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, hidden_neuron in enumerate(network[0]):\n",
    "        for j, input in enumerate(input_vector + [1]):\n",
    "            hidden_neuron[j] -= hidden_deltas[i] * input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_neuron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets =[ [1 if i==j else 0 for i in range(10)] for j in range(10) ]\n",
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(0)\n",
    "input_size = 25\n",
    "num_hidden = 5\n",
    "output_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_layer = [ [random.random() for _ in range(input_size + 1)] for _ in range(num_hidden) ]\n",
    "hidden_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_layer = [ [random.random() for _ in range(num_hidden + 1)] for _ in range(output_size) ]\n",
    "output_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = [hidden_layer, output_layer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_digits = [\n",
    "          \"\"\"11111\n",
    "             1...1\n",
    "             1...1\n",
    "             1...1\n",
    "             11111\"\"\",\n",
    "\n",
    "          \"\"\"..1..\n",
    "             ..1..\n",
    "             ..1..\n",
    "             ..1..\n",
    "             ..1..\"\"\",\n",
    "\n",
    "          \"\"\"11111\n",
    "             ....1\n",
    "             11111\n",
    "             1....\n",
    "             11111\"\"\",\n",
    "\n",
    "          \"\"\"11111\n",
    "             ....1\n",
    "             11111\n",
    "             ....1\n",
    "             11111\"\"\",\n",
    "\n",
    "          \"\"\"1...1\n",
    "             1...1\n",
    "             11111\n",
    "             ....1\n",
    "             ....1\"\"\",\n",
    "\n",
    "          \"\"\"11111\n",
    "             1....\n",
    "             11111\n",
    "             ....1\n",
    "             11111\"\"\",\n",
    "\n",
    "          \"\"\"11111\n",
    "             1....\n",
    "             11111\n",
    "             1...1\n",
    "             11111\"\"\",\n",
    "\n",
    "          \"\"\"11111\n",
    "             ....1\n",
    "             ....1\n",
    "             ....1\n",
    "             ....1\"\"\",\n",
    "\n",
    "          \"\"\"11111\n",
    "             1...1\n",
    "             11111\n",
    "             1...1\n",
    "             11111\"\"\",\n",
    "\n",
    "          \"\"\"11111\n",
    "             1...1\n",
    "             11111\n",
    "             ....1\n",
    "             11111\"\"\"]\n",
    "\n",
    "def make_digit(raw_digit):\n",
    "    return [1 if c == '1' else 0\n",
    "        for row in raw_digit.split(\"\\n\")\n",
    "        for c in row.strip()]\n",
    "\n",
    "inputs = list(map(make_digit, raw_digits)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(10000):\n",
    "    for input_vector, target_vector in zip(inputs, targets):\n",
    "        backpropagate(network, input_vector, target_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(input):\n",
    "    return feed_forward(network, input)[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(inputs[7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def patch(x, y, hatch, color):\n",
    "    \"\"\"вернуть объект 'patch' библиотеки matplotlib с указанными\n",
    "    координатами, шаблоном штриховки и цветом\"\"\"\n",
    "    return matplotlib.patches.Rectangle((x - 0.5, y - 0.5), 1, 1,\n",
    "                                        hatch=hatch, fill=False, color=color)\n",
    "\n",
    "\n",
    "def show_weights(neuron_idx):\n",
    "    weights = network[0][neuron_idx]\n",
    "    abs_weights = list(map(abs, weights)) #UPD в оригинале map(abs, weights)\n",
    "\n",
    "    grid = [abs_weights[row:(row+5)]     # преобразовать веса в матрицу 5 x 5 \n",
    "            for row in range(0,25,5)]    # [weights[0:5], ..., weights[20:25]]\n",
    "\n",
    "    ax = plt.gca()                       # для штриховки нужен объект axis (ось)\n",
    "\n",
    "    ax.imshow(grid,                      # здесь так же как в plt.imshow\n",
    "              cmap=matplotlib.cm.binary, # использовать шкалу ч/б цвета\n",
    "              interpolation='none')      # рисовать блоками\n",
    "\n",
    "    # заштриховать отрицательные веса\n",
    "    for i in range(5):               # строка\n",
    "        for j in range(5):           # столбец\n",
    "            if weights[5*i + j] < 0: # строка i, столбец j = weights[5*i + j]\n",
    "                # добавить ч/б штриховку, чтобы было видно на темном или светлом\n",
    "                ax.add_patch(patch(j, i, '/', \"white\"))\n",
    "                ax.add_patch(patch(j, i, '\\\\', \"black\"))\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_weights(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# КЛАСТЕРИЗАЦИЯ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KMeans:\n",
    "    \"\"\"класс выполняет кластеризацию по методу k-средних\"\"\"\n",
    "\n",
    "    def __init__(self, k):\n",
    "        self.k = k          # число кластеров\n",
    "        self.means = None   # средние кластеров\n",
    "\n",
    "    def classify(self, input):\n",
    "        \"\"\"вернуть индекс кластера, ближайшего к входящему значению input\"\"\"\n",
    "        return min(range(self.k),\n",
    "                   key=lambda i: squared_distance(input, self.means[i]))\n",
    "\n",
    "    def train(self, inputs):\n",
    "\n",
    "        self.means = random.sample(inputs, self.k)\n",
    "        assignments = None\n",
    "\n",
    "        while True:\n",
    "            # найти новые назначения\n",
    "            new_assignments = list(map(self.classify, inputs))\n",
    "\n",
    "            # если ни одно назначение не изменилось, то завершить\n",
    "            if assignments == new_assignments:\n",
    "                return\n",
    "\n",
    "            # в противном случае сохранить новые назначения\n",
    "            assignments = new_assignments\n",
    "\n",
    "            for i in range(self.k):\n",
    "                i_points = [p for p, a in zip(inputs, assignments) if a == i]\n",
    "                # удостовериться, что i_points не пуст, чтобы не делить на 0\n",
    "                if i_points:\n",
    "                    self.means[i] = vector_mean(i_points)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [[-14,-5],[13,13],[20,23],[-19,-11],[-9,-16],\n",
    "              [21,27],[-49,15],[26,13],[-46,5],[-34,-1],\n",
    "              [11,15],[-49,0],[-22,-16],[19,28],[-12,-8],\n",
    "              [-13,-19],[-41,8],[-11,-6],[-25,-9],[-18,-3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(0)\n",
    "clusterer = KMeans(3)\n",
    "clusterer.train(inputs)\n",
    "print(clusterer.means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(0)\n",
    "clusterer = KMeans(2)\n",
    "clusterer.train(inputs)\n",
    "print(clusterer.means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def squared_clustering_errors(inputs, k):\n",
    "    \"\"\"находит суммарное квадратичное отклонение (ошибку) \n",
    "    от k-средних при кластеризации входящих данных\"\"\"\n",
    "    clusterer = KMeans(k)\n",
    "    clusterer.train(inputs)\n",
    "    means = clusterer.means\n",
    "    assignments = list(map(clusterer.classify, inputs))\n",
    "\n",
    "    return sum(squared_distance(input,means[cluster])\n",
    "               for input, cluster in zip(inputs, assignments))\n",
    "\n",
    "def plot_squared_clustering_errors():\n",
    "\n",
    "    ks = range(1, len(inputs) + 1)\n",
    "    errors = [squared_clustering_errors(inputs, k) for k in ks]\n",
    "\n",
    "    plt.plot(ks, errors)\n",
    "    plt.xticks(ks)\n",
    "    plt.xlabel(\"k\")\n",
    "    plt.ylabel(\"Суммарнное квадратичное отклонение\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_squared_clustering_errors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assignments = list(map(clusterer.classify, inputs))\n",
    "assignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recolor_image(k=5):\n",
    "\n",
    "    img = mpimg.imread('C:/Users/user/Documents/image_example.png')\n",
    "    pixels = [pixel for row in img for pixel in row]\n",
    "    clusterer = KMeans(k)\n",
    "    clusterer.train(pixels) # обучение может занять некоторое время\n",
    "\n",
    "    def recolor(pixel):\n",
    "        cluster = clusterer.classify(pixel) # индекс ближайшего кластера\n",
    "        return clusterer.means[cluster]     # среднее ближайшего кластера\n",
    "\n",
    "    new_img = [[recolor(pixel) for pixel in row]\n",
    "               for row in img]\n",
    "\n",
    "    plt.imshow(new_img)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = mpimg.imread('C:/Users/user/Documents/image_example.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ОБРАБОТКА ЕСТЕСТВЕННОГО ЯЗЫКА"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_resumes(plt):\n",
    "    data = [ (\"big data\", 100, 15), (\"Hadoop\", 95, 25), (\"Python\", 75, 50),\n",
    "         (\"R\", 50, 40), (\"machine learning\", 80, 20), (\"statistics\", 20, 60),\n",
    "         (\"data science\", 60, 70), (\"analytics\", 90, 3),\n",
    "         (\"team player\", 85, 85), (\"dynamic\", 2, 90), (\"synergies\", 70, 0),\n",
    "         (\"actionable insights\", 40, 30), (\"think out of the box\", 45, 10),\n",
    "         (\"self-starter\", 30, 50), (\"customer focus\", 65, 15),\n",
    "         (\"thought leadership\", 35, 35)]\n",
    "\n",
    "    def text_size(total):\n",
    "        \"\"\"равно 8, если total = 0, и 28, если total = 200\"\"\"\n",
    "        return 8 + total / 200 * 20\n",
    "\n",
    "    for word, job_popularity, resume_popularity in data:\n",
    "        plt.text(job_popularity, resume_popularity, word,\n",
    "                 ha='center', va='center',\n",
    "                 size=text_size(job_popularity + resume_popularity))\n",
    "    plt.xlabel(\"Популярность среди объявлений о вакансиях\")\n",
    "    plt.ylabel(\"Популярность среди резюме\")\n",
    "    plt.axis([0, 100, 0, 100])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_resumes(plt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_unicode(text):\n",
    "    return text.replace(u\"\\u2019\", \"'\")\n",
    "\n",
    "def get_document():\n",
    "\n",
    "    url = \"http://radar.oreilly.com/2010/06/what-is-data-science.html\"\n",
    "    html = requests.get(url).text\n",
    "    soup = BeautifulSoup(html, 'html.parser')   #UPD в оригинале 'html5lib'\n",
    "\n",
    "    content = soup.find(\"div\", \"article-body\")  # найти div с классом entry-content\n",
    "    regex = r\"[\\w']+|[\\.]\"                      # совпадает со словом или точкой\n",
    "\n",
    "    document = []\n",
    "\n",
    "\n",
    "    for paragraph in content(\"p\"):\n",
    "        words = re.findall(regex, fix_unicode(paragraph.text))\n",
    "        document.extend(words)\n",
    "\n",
    "    return document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"http://radar.oreilly.com/2010/06/what-is-data-science.html\"\n",
    "html = requests.get(url).text\n",
    "html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "content = soup.find(\"div\", \"article-body\")\n",
    "content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document=get_document()\n",
    "document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams = zip(document, document[1:])\n",
    "bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transitions = defaultdict(list)\n",
    "for prev, current in bigrams:\n",
    "    transitions[prev].append(current)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_=[2,1,1,3,1,5,6,2,2,1]\n",
    "bigrams_ = zip(document_, document_[1:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for prev, current in bigrams_:\n",
    "    print(prev, current)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transitions_ = defaultdict(list)\n",
    "for prev, current in bigrams_:\n",
    "    transitions_[prev].append(current)\n",
    "transitions_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transitions['.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_using_bigrams():\n",
    "    current = '.'\n",
    "    result = []\n",
    "    while True:\n",
    "        next_word_candidates = transitions[current]\n",
    "        current = random.choice(next_word_candidates)\n",
    "        result.append(current)\n",
    "        if current == '.':\n",
    "            return \" \".join(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_using_bigrams()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_using_bigrams()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "generate_using_bigrams()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigrams = list(zip(document, document[1:], document[2:]))\n",
    "trigram_transitions = defaultdict(list)\n",
    "starts = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for prev, current, next in trigrams:\n",
    "\n",
    "        if prev == \".\":              # if the previous \"word\" was a period\n",
    "            starts.append(current)   # then this is a start word\n",
    "\n",
    "        trigram_transitions[(prev, current)].append(next)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_using_trigrams(starts, trigram_transitions):\n",
    "    current = random.choice(starts)   # выбрать произвольное исхолное слово\n",
    "    prev = \".\"                        # и поставить перед ним '.'\n",
    "    result = [current]\n",
    "    while True:\n",
    "        next_word_candidates = trigram_transitions[(prev, current)]\n",
    "        next = random.choice(next_word_candidates)\n",
    "\n",
    "        prev, current = current, next\n",
    "        result.append(current)\n",
    "\n",
    "        if current == \".\":\n",
    "            return \" \".join(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_using_trigrams(starts, trigram_transitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_using_trigrams(starts, trigram_transitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_using_trigrams(starts, trigram_transitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar = {\n",
    "        \"_S\"  : [\"_NP _VP\"],\n",
    "        \"_NP\" : [\"_N\",\n",
    "                 \"_A _NP _P _A _N\"],\n",
    "        \"_VP\" : [\"_V\",\n",
    "                 \"_V _NP\"],\n",
    "        \"_N\"  : [\"data science\", \"Python\", \"regression\"],\n",
    "        \"_A\"  : [\"big\", \"linear\", \"logistic\"],\n",
    "        \"_P\"  : [\"about\", \"near\"],\n",
    "        \"_V\"  : [\"learns\", \"trains\", \"tests\", \"is\"]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_terminal(token):\n",
    "    return token[0] != \"_\"\n",
    "\n",
    "def expand(grammar, tokens):\n",
    "    for i, token in enumerate(tokens):\n",
    "\n",
    "        # пропустить терминалы\n",
    "        if is_terminal(token): continue\n",
    "\n",
    "        # choose a replacement at random\n",
    "        replacement = random.choice(grammar[token])\n",
    "\n",
    "        if is_terminal(replacement):\n",
    "            tokens[i] = replacement\n",
    "        else:\n",
    "            tokens = tokens[:i] + replacement.split() + tokens[(i+1):]\n",
    "        return expand(grammar, tokens)\n",
    "\n",
    "    # если мы тут, значит, нашли нетерминальную лексему,\n",
    "    # произвольно выбрать для нее подстановку\n",
    "    return tokens\n",
    "\n",
    "def generate_sentence(grammar):\n",
    "    return expand(grammar, [\"_S\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_sentence(grammar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_sentence(grammar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "generate_sentence(grammar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "тематическое моделирование"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def roll_a_die():\n",
    "    return random.choice([1,2,3,4,5,6])\n",
    "\n",
    "def direct_sample():\n",
    "    d1 = roll_a_die()\n",
    "    d2 = roll_a_die()\n",
    "    return d1, d1 + d2\n",
    "\n",
    "def random_y_given_x(x):\n",
    "    \"\"\"равновероятное значение будет x + 1, x + 2, ... , x + 6\"\"\"\n",
    "    return x + roll_a_die()\n",
    "\n",
    "def random_x_given_y(y):\n",
    "    if y <= 7:\n",
    "        # если сумма <= 7, первый кубик равновероятно будет равен\n",
    "        # 1, 2, ..., (сумма - 1)\n",
    "        return random.randrange(1, y)\n",
    "    else:\n",
    "        # если сумма > 7, первый кубик равновероятно будет равен\n",
    "        # (сумма - 6), (сумма - 5), ..., 6\n",
    "        return random.randrange(y - 6, 7)\n",
    "\n",
    "def gibbs_sample(num_iters=100):\n",
    "    x, y = 1, 2 # на самом деле не имеет значения, какие числа\n",
    "    for _ in range(num_iters):\n",
    "        x = random_x_given_y(y)\n",
    "        y = random_y_given_x(x)\n",
    "    return x, y\n",
    "\n",
    "def compare_distributions(num_samples=1000):\n",
    "    counts = defaultdict(lambda: [0, 0])\n",
    "    for _ in range(num_samples):\n",
    "        counts[gibbs_sample()][0] += 1\n",
    "        counts[direct_sample()][1] += 1\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_distributions(num_samples=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\n",
    "    [\"Hadoop\", \"Big Data\", \"HBase\", \"Java\", \"Spark\", \"Storm\", \"Cassandra\"],\n",
    "    [\"NoSQL\", \"MongoDB\", \"Cassandra\", \"HBase\", \"Postgres\"],\n",
    "    [\"Python\", \"scikit-learn\", \"scipy\", \"numpy\", \"statsmodels\", \"pandas\"],\n",
    "    [\"R\", \"Python\", \"statistics\", \"regression\", \"probability\"],\n",
    "    [\"machine learning\", \"regression\", \"decision trees\", \"libsvm\"],\n",
    "    [\"Python\", \"R\", \"Java\", \"C++\", \"Haskell\", \"programming languages\"],\n",
    "    [\"statistics\", \"probability\", \"mathematics\", \"theory\"],\n",
    "    [\"machine learning\", \"scikit-learn\", \"Mahout\", \"neural networks\"],\n",
    "    [\"neural networks\", \"deep learning\", \"Big Data\", \"artificial intelligence\"],\n",
    "    [\"Hadoop\", \"Java\", \"MapReduce\", \"Big Data\"],\n",
    "    [\"statistics\", \"R\", \"statsmodels\"],\n",
    "    [\"C++\", \"deep learning\", \"artificial intelligence\", \"probability\"],\n",
    "    [\"pandas\", \"R\", \"Python\"],\n",
    "    [\"databases\", \"HBase\", \"Postgres\", \"MySQL\", \"MongoDB\"],\n",
    "    [\"libsvm\", \"regression\", \"support vector machines\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_from(weights):\n",
    "    total = sum(weights)\n",
    "    rnd = total * random.random()       # равномерно между 0 и суммой\n",
    "    for i, w in enumerate(weights):\n",
    "        rnd -= w                        # вернуть наименьший i, такой что\n",
    "        if rnd <= 0: return i           # sum(weights[:(i+1)]) >= rnd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_topic_counts = [Counter()\n",
    "                         for _ in documents]\n",
    "\n",
    "topic_word_counts = [Counter() for _ in range(K)]\n",
    "\n",
    "topic_counts = [0 for _ in range(K)]\n",
    "\n",
    "document_lengths = [len(d) for d in documents]\n",
    "\n",
    "distinct_words = set(word for document in documents for word in document)\n",
    "W = len(distinct_words)\n",
    "\n",
    "D = len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distinct_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def p_topic_given_document(topic, d, alpha=0.1):\n",
    "    \"\"\"доля слов в документе _d_, которые\n",
    "    назначаются тематике _topic_ (плюс некоторое сглаживание)\"\"\"\n",
    "\n",
    "    return ((document_topic_counts[d][topic] + alpha) /\n",
    "            (document_lengths[d] + K * alpha))\n",
    "\n",
    "def p_word_given_topic(word, topic, beta=0.1):\n",
    "    \"\"\"доля слов, назначаемых тематике _topic_, которые\n",
    "    равны _word_ (плюс некоторое сглаживание)\"\"\"\n",
    "\n",
    "    return ((topic_word_counts[topic][word] + beta) /\n",
    "            (topic_counts[topic] + W * beta))\n",
    "\n",
    "def topic_weight(d, word, k):\n",
    "    \"\"\"при наличии документа и слова в этом документе,\n",
    "    вернуть вес k-ой темы\"\"\"\n",
    "\n",
    "    return p_word_given_topic(word, k) * p_topic_given_document(k, d)\n",
    "\n",
    "def choose_new_topic(d, word):\n",
    "    return sample_from([topic_weight(d, word, k)\n",
    "                        for k in range(K)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(0)\n",
    "document_topics = [[random.randrange(K) for word in document]\n",
    "                   for document in documents]\n",
    "\n",
    "for d in range(D):\n",
    "    for word, topic in zip(documents[d], document_topics[d]):\n",
    "        document_topic_counts[d][topic] += 1\n",
    "        topic_word_counts[topic][word] += 1\n",
    "        topic_counts[topic] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for iter in range(1000):\n",
    "    for d in range(D):\n",
    "        for i, (word, topic) in enumerate(zip(documents[d],\n",
    "                                              document_topics[d])):\n",
    "\n",
    "            # удалить это слово / тематику из показателей,\n",
    "            # чтобы оно не влияло на веса\n",
    "            document_topic_counts[d][topic] -= 1\n",
    "            topic_word_counts[topic][word] -= 1\n",
    "            topic_counts[topic] -= 1\n",
    "            document_lengths[d] -= 1\n",
    "\n",
    "            # выбрать новую тематику на основе весов\n",
    "            new_topic = choose_new_topic(d, word)\n",
    "            document_topics[d][i] = new_topic\n",
    "\n",
    "            # и теперь снова увеличить показатели\n",
    "            document_topic_counts[d][new_topic] += 1\n",
    "            topic_word_counts[new_topic][word] += 1\n",
    "            topic_counts[new_topic] += 1\n",
    "            document_lengths[d] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, word_counts in enumerate(topic_word_counts):\n",
    "        for word, count in word_counts.most_common():\n",
    "            if count > 0: print(k, word, count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    topic_names = [\"Big Data and programming languages\",\n",
    "                   \"databases\",\n",
    "                   \"machine learning\",\n",
    "                   \"statistics\"]\n",
    "\n",
    "    for document, topic_counts in zip(documents, document_topic_counts):\n",
    "        print(document)\n",
    "        for topic, count in topic_counts.most_common():\n",
    "            if count > 0:\n",
    "                print(topic_names[topic], count)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk as nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> import nltk\n",
    ">>> sentence = \"\"\"At eight o'clock on Thursday morning\n",
    "... Arthur didn't feel very good.\"\"\"\n",
    ">>> tokens = nltk.word_tokenize(sentence)\n",
    ">>> tokens\n",
    "['At', 'eight', \"o'clock\", 'on', 'Thursday', 'morning',\n",
    "'Arthur', 'did', \"n't\", 'feel', 'very', 'good', '.']\n",
    ">>> tagged = nltk.pos_tag(tokens)\n",
    ">>> tagged[0:6]\n",
    "[('At', 'IN'), ('eight', 'CD'), (\"o'clock\", 'JJ'), ('on', 'IN'),\n",
    "('Thursday', 'NNP'), ('morning', 'NN')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"\"\"At eight o'clock on Thursday morning\n",
    "... Arthur didn't feel very good.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = nltk.word_tokenize(sentence)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged = nltk.pos_tag(tokens)\n",
    "tagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# РЕКОМЕНДАТЕЛЬНЫЕ СИСТЕМЫ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_interests = [\n",
    "    [\"Hadoop\", \"Big Data\", \"HBase\", \"Java\", \"Spark\", \"Storm\", \"Cassandra\"],\n",
    "    [\"NoSQL\", \"MongoDB\", \"Cassandra\", \"HBase\", \"Postgres\"],\n",
    "    [\"Python\", \"scikit-learn\", \"scipy\", \"numpy\", \"statsmodels\", \"pandas\"],\n",
    "    [\"R\", \"Python\", \"statistics\", \"regression\", \"probability\"],\n",
    "    [\"machine learning\", \"regression\", \"decision trees\", \"libsvm\"],\n",
    "    [\"Python\", \"R\", \"Java\", \"C++\", \"Haskell\", \"programming languages\"],\n",
    "    [\"statistics\", \"probability\", \"mathematics\", \"theory\"],\n",
    "    [\"machine learning\", \"scikit-learn\", \"Mahout\", \"neural networks\"],\n",
    "    [\"neural networks\", \"deep learning\", \"Big Data\", \"artificial intelligence\"],\n",
    "    [\"Hadoop\", \"Java\", \"MapReduce\", \"Big Data\"],\n",
    "    [\"statistics\", \"R\", \"statsmodels\"],\n",
    "    [\"C++\", \"deep learning\", \"artificial intelligence\", \"probability\"],\n",
    "    [\"pandas\", \"R\", \"Python\"],\n",
    "    [\"databases\", \"HBase\", \"Postgres\", \"MySQL\", \"MongoDB\"],\n",
    "    [\"libsvm\", \"regression\", \"support vector machines\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "popular_interests = Counter(interest for user_interests in users_interests for interest in user_interests).most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "popular_interests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_popular_new_interests(user_interests, max_results=5):\n",
    "    suggestions = [(interest, frequency)\n",
    "                   for interest, frequency in popular_interests\n",
    "                   if interest not in user_interests]\n",
    "    return suggestions[:max_results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_interests = [\"NoSQL\", \"MongoDB\", \"Cassandra\", \"HBase\", \"Postgres\"]\n",
    "most_popular_new_interests(user_interests, max_results=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# коллаборативная фильтрация на основе пользователя\n",
    "#\n",
    "\n",
    "def cosine_similarity(v, w):\n",
    "    return dot(v, w) / math.sqrt(dot(v, v) * dot(w, w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_interests = sorted(list({ interest\n",
    "                                 for user_interests in users_interests\n",
    "                                 for interest in user_interests }))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_interests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_user_interest_vector(user_interests):\n",
    "    \"\"\"при заданном списке интересующих пользователя тем создать вектор,\n",
    "    чей i-ый элемент равен 1, если unique_interests[i] есть в списке,\n",
    "    и 0 в противном случае\"\"\"\n",
    "    return [1 if interest in user_interests else 0\n",
    "            for interest in unique_interests]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_interest_matrix = list(map(make_user_interest_vector, users_interests))\n",
    "user_interest_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_interest_matrix[0] # - предпочтения пользователя 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_similarities = [[cosine_similarity(interest_vector_i, interest_vector_j)\n",
    "                      for interest_vector_j in user_interest_matrix]\n",
    "                     for interest_vector_i in user_interest_matrix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_similar_users_to(user_id):\n",
    "    pairs = [(other_user_id, similarity)                      # найти других\n",
    "             for other_user_id, similarity in                 # пользователей с\n",
    "                enumerate(user_similarities[user_id])         # ненулевым коэфф.\n",
    "             if user_id != other_user_id and similarity > 0]  # подобия\n",
    "\n",
    "    return sorted(pairs,                                      # отсортировать их\n",
    "                  key=lambda pair: pair[1],                   # сперва наиболее\n",
    "                  reverse=True)                               # похожие\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_similar_users_to(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_based_suggestions(user_id, include_current_interests=False):\n",
    "    # суммировать все коэффициенты подобия\n",
    "    suggestions = defaultdict(float)\n",
    "    for other_user_id, similarity in most_similar_users_to(user_id):\n",
    "        for interest in users_interests[other_user_id]:\n",
    "            suggestions[interest] += similarity\n",
    "\n",
    "    # преобразовать их в сортированный список\n",
    "    suggestions = sorted(suggestions.items(),\n",
    "                         key=lambda pair: pair[1],\n",
    "                         reverse=True)\n",
    "\n",
    "    # и (может быть) исключить уже имеющиеся темы\n",
    "    if include_current_interests:\n",
    "        return suggestions\n",
    "    else:\n",
    "        return [(suggestion, weight)\n",
    "                for suggestion, weight in suggestions\n",
    "                if suggestion not in users_interests[user_id]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_based_suggestions(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# коллаборативная фильтрация по схожести предметов\n",
    "#\n",
    "\n",
    "interest_user_matrix = [[user_interest_vector[j]\n",
    "                         for user_interest_vector in user_interest_matrix]\n",
    "                        for j, _ in enumerate(unique_interests)]\n",
    "\n",
    "interest_similarities = [[cosine_similarity(user_vector_i, user_vector_j)\n",
    "                          for user_vector_j in interest_user_matrix]\n",
    "                         for user_vector_i in interest_user_matrix]\n",
    "\n",
    "def most_similar_interests_to(interest_id):\n",
    "    similarities = interest_similarities[interest_id]\n",
    "    pairs = [(unique_interests[other_interest_id], similarity)\n",
    "             for other_interest_id, similarity in enumerate(similarities)\n",
    "             if interest_id != other_interest_id and similarity > 0]\n",
    "    return sorted(pairs,\n",
    "                  key=lambda pair: pair[1],\n",
    "                  reverse=True)\n",
    "\n",
    "def item_based_suggestions(user_id, include_current_interests=False):\n",
    "    suggestions = defaultdict(float)\n",
    "    user_interest_vector = user_interest_matrix[user_id]\n",
    "    for interest_id, is_interested in enumerate(user_interest_vector):\n",
    "        if is_interested == 1:\n",
    "            similar_interests = most_similar_interests_to(interest_id)\n",
    "            for interest, similarity in similar_interests:\n",
    "                suggestions[interest] += similarity\n",
    "\n",
    "    suggestions = sorted(suggestions.items(),\n",
    "                         key=lambda pair: pair[1],\n",
    "                         reverse=True)\n",
    "\n",
    "    if include_current_interests:\n",
    "        return suggestions\n",
    "    else:\n",
    "        return [(suggestion, weight)\n",
    "                for suggestion, weight in suggestions\n",
    "                if suggestion not in users_interests[user_id]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_similar_interests_to(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "item_based_suggestions(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# БАЗЫ ДАННЫХ И SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# РАСПРЕДЕЛЕННЫЕ ВЫЧИСЛЕНИЯ MAPREDUCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_count_old(documents):\n",
    "    \"\"\"подсчет частотности слов без использования технологии MapReduce\"\"\"\n",
    "    return Counter(word\n",
    "        for document in documents\n",
    "        for word in tokenize(document))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wc_mapper(document):\n",
    "    \"\"\"для каждого слова в документе, генерировать (слово,1)\"\"\"\n",
    "    for word in tokenize(document):\n",
    "        yield (word, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = ('They can you need to', 'go to be current to collect and coupled to fill the companies that', 'would start a divide and find new products incrementally the results in advance conflicts with human language and other members of Moores Law comes in not as Dataspaces by providing one in your data was someone on')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count_old(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wc_reducer(word, counts):\n",
    "    \"\"\"суммировать количества появлений слова\"\"\"\n",
    "    yield (word, sum(counts))\n",
    "\n",
    "def word_count(documents):\n",
    "    \"\"\"подсчитать слова во входящих документах при помощи MapReduce\"\"\"\n",
    "\n",
    "    # место для хранения сгруппированных значений\n",
    "    collector = defaultdict(list)\n",
    "\n",
    "    for document in documents:\n",
    "        for word, count in wc_mapper(document):\n",
    "            collector[word].append(count)\n",
    "\n",
    "    return [output\n",
    "            for word, counts in collector.items()\n",
    "            for output in wc_reducer(word, counts)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_reduce(inputs, mapper, reducer):\n",
    "    \"\"\"применяет MapReduce ко входящим данным, используя\n",
    "       проектор mapper и редуктор reducer\"\"\"\n",
    "    collector = defaultdict(list)\n",
    "\n",
    "    for input in inputs:\n",
    "        for key, value in mapper(input):\n",
    "            collector[key].append(value)\n",
    "\n",
    "    return [output\n",
    "            for key, values in collector.items()\n",
    "            for output in reducer(key,values)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_reduce(documents, wc_mapper, wc_reducer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SOMETHING ABOUT SQL & PYTHON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect('Chinook_Sqlite.sqlite')\n",
    "conn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.execute(\"SELECT Name FROM Artist ORDER BY Name LIMIT 3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = cursor.fetchall()\n",
    "results2 =  cursor.fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.execute(\"insert into Artist values (Null, 'A Aagrh!') \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.execute(\"SELECT Name FROM Artist ORDER BY Name LIMIT 3\")\n",
    "results = cursor.fetchall()\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.execute(\"\"\"\n",
    "  SELECT name\n",
    "  FROM Artist\n",
    "  ORDER BY Name LIMIT 3\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
