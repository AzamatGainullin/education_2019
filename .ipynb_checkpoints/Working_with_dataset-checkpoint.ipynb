{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#импорт необходимых библиотек\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import time\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import os\n",
    "import copy\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "from sklearn import decomposition\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import ListedColormap\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_decision_regions(X, y, classifier, test_idx=None, resolution=0.02):\n",
    "\n",
    "    # setup marker generator and color map\n",
    "    markers = ('s', 'x', 'o', '^', 'v')\n",
    "    colors = ('red', 'blue', 'lightgreen', 'gray', 'cyan')\n",
    "    cmap = ListedColormap(colors[:len(np.unique(y))])\n",
    "\n",
    "    # plot the decision surface\n",
    "    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),\n",
    "                           np.arange(x2_min, x2_max, resolution))\n",
    "    Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)\n",
    "    Z = Z.reshape(xx1.shape)\n",
    "    plt.contourf(xx1, xx2, Z, alpha=0.3, cmap=cmap)\n",
    "    plt.xlim(xx1.min(), xx1.max())\n",
    "    plt.ylim(xx2.min(), xx2.max())\n",
    "\n",
    "    for idx, cl in enumerate(np.unique(y)):\n",
    "        plt.scatter(x=X[y == cl, 0], \n",
    "                    y=X[y == cl, 1],\n",
    "                    alpha=0.8, \n",
    "                    c=colors[idx],\n",
    "                    marker=markers[idx], \n",
    "                    label=cl, \n",
    "                    edgecolor='black')\n",
    "\n",
    "    # highlight test samples\n",
    "    if test_idx:\n",
    "        # plot all samples\n",
    "        X_test, y_test = X[test_idx, :], y[test_idx]\n",
    "\n",
    "        plt.scatter(X_test[:, 0],\n",
    "                    X_test[:, 1],\n",
    "                    c='',\n",
    "                    edgecolor='black',\n",
    "                    alpha=1.0,\n",
    "                    linewidth=1,\n",
    "                    marker='o',\n",
    "                    s=100, \n",
    "                    label='test set')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Чтение и нормирование X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "marked_dataset = pd.read_excel('marked_dataset_old.xlsx')\n",
    "marked_dataset.drop(columns='class', inplace=True)\n",
    "ready_dataset_for_marking_last = pd.read_excel('ready_dataset_for_marking_last.xlsx')\n",
    "\n",
    "marked_dataset = marked_dataset.append(ready_dataset_for_marking_last)\n",
    "marked_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#md = pd.read_excel('принцип_работы_рандом форрест классифайер.xlsx')\n",
    "#md.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list(md['md.columns'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X = marked_dataset[list(md['md.columns'])]\n",
    "X = marked_dataset.drop(columns='class')\n",
    "\n",
    "Y = marked_dataset['class']\n",
    "#Y = (Y==2)\n",
    "#Y = (Y==0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, stratify=Y, random_state=25, test_size=0.06)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ЖИЗНЕННО ВАЖНОЕ ШКАЛИРОВАНИЕ/НОРМИРОВАНИЕ ДАННЫХ\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pca = decomposition.PCA(n_components=30)\n",
    "#pca.fit(X_train)\n",
    "#X_train = pca.transform(X_train)\n",
    "#X_test=pca.transform(X_test)\n",
    "#model = RandomForestClassifier(random_state=40, class_weight={False:0.85, True:0.15},n_estimators=100)\n",
    "#model = model\n",
    "#y_train_pred = cross_val_predict(model, Xtrain, y_train, cv=4)\n",
    "#conf = confusion_matrix(y_train, y_train_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ковариционные матрицы, понижение размерности"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "cov_mat = np.cov(X_train_std.T)\n",
    "#cov_mat = np.cov(X.T)\n",
    "eigen_vals, eigen_vecs = np.linalg.eig(cov_mat)\n",
    "\n",
    "print('\\nEigenvalues \\n%s' % eigen_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot = sum(eigen_vals)\n",
    "var_exp = [(i / tot) for i in sorted(eigen_vals, reverse=True)]\n",
    "cum_var_exp = np.cumsum(var_exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "plt.bar(range(1, 29), var_exp, alpha=0.5, align='center',\n",
    "        label='individual explained variance')\n",
    "plt.step(range(1, 29), cum_var_exp, where='mid',\n",
    "         label='cumulative explained variance')\n",
    "plt.ylabel('Explained variance ratio')\n",
    "plt.xlabel('Principal component index')\n",
    "plt.legend(loc='best')\n",
    "plt.tight_layout()\n",
    "# plt.savefig('images/05_02.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=4)\n",
    "\n",
    "mean_vecs = []\n",
    "for label in range(0, 3):\n",
    "    mean_vecs.append(np.mean(X_train_std[y_train == label], axis=0))\n",
    "    print('MV %s: %s\\n' % (label, mean_vecs[label - 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[mean_vecs[0][i] / mean_vecs[1][i] for i in range(len(mean_vecs[0]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = 224 # number of features\n",
    "S_W = np.zeros((d, d))\n",
    "for label, mv in zip(range(0, 3), mean_vecs):\n",
    "    class_scatter = np.zeros((d, d))  # scatter matrix for each class\n",
    "    for row in X_train_std[y_train == label]:\n",
    "        row, mv = row.reshape(d, 1), mv.reshape(d, 1)  # make column vectors\n",
    "        class_scatter += (row - mv).dot((row - mv).T)\n",
    "    S_W += class_scatter                          # sum class scatter matrices\n",
    "\n",
    "print('Within-class scatter matrix: %sx%s' % (S_W.shape[0], S_W.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S_W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Class label distribution: %s' \n",
    "      % np.bincount(y_train)[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = 224  # number of features\n",
    "S_W = np.zeros((d, d))\n",
    "for label, mv in zip(range(0, 3), mean_vecs):\n",
    "    class_scatter = np.cov(X_train_std[y_train == label].T)\n",
    "    S_W += class_scatter\n",
    "print('Scaled within-class scatter matrix: %sx%s' % (S_W.shape[0],\n",
    "                                                     S_W.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_overall = np.mean(X_train_std, axis=0)\n",
    "d = 224  # number of features\n",
    "S_B = np.zeros((d, d))\n",
    "for i, mean_vec in enumerate(mean_vecs):\n",
    "    n = X_train_std[y_train == i + 1, :].shape[0]\n",
    "    mean_vec = mean_vec.reshape(d, 1)  # make column vector\n",
    "    mean_overall = mean_overall.reshape(d, 1)  # make column vector\n",
    "    S_B += n * (mean_vec - mean_overall).dot((mean_vec - mean_overall).T)\n",
    "\n",
    "print('Between-class scatter matrix: %sx%s' % (S_B.shape[0], S_B.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S_B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eigen_vals, eigen_vecs = np.linalg.eig(np.linalg.inv(S_W).dot(S_B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a list of (eigenvalue, eigenvector) tuples\n",
    "eigen_pairs = [(np.abs(eigen_vals[i]), eigen_vecs[:, i])\n",
    "               for i in range(len(eigen_vals))]\n",
    "\n",
    "# Sort the (eigenvalue, eigenvector) tuples from high to low\n",
    "eigen_pairs = sorted(eigen_pairs, key=lambda k: k[0], reverse=True)\n",
    "\n",
    "# Visually confirm that the list is correctly sorted by decreasing eigenvalues\n",
    "\n",
    "print('Eigenvalues in descending order:\\n')\n",
    "for eigen_val in eigen_pairs:\n",
    "    print(eigen_val[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot = sum(eigen_vals.real)\n",
    "discr = [(i / tot) for i in sorted(eigen_vals.real, reverse=True)]\n",
    "cum_discr = np.cumsum(discr)\n",
    "\n",
    "plt.bar(range(1, 225), discr, alpha=0.5, align='center',\n",
    "        label='individual \"discriminability\"')\n",
    "plt.step(range(1, 225), cum_discr, where='mid',\n",
    "         label='cumulative \"discriminability\"')\n",
    "plt.ylabel('\"discriminability\" ratio')\n",
    "plt.xlabel('Linear Discriminants')\n",
    "plt.ylim([-0.1, 1.1])\n",
    "plt.legend(loc='best')\n",
    "plt.tight_layout()\n",
    "# plt.savefig('images/05_07.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.hstack((eigen_pairs[0][1][:, np.newaxis].real,\n",
    "              eigen_pairs[1][1][:, np.newaxis].real))\n",
    "print('Matrix W:\\n', w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_lda = X_train_std.dot(w)\n",
    "colors = ['r', 'b', 'g']\n",
    "markers = ['s', 'x', 'o']\n",
    "\n",
    "for l, c, m in zip(np.unique(y_train), colors, markers):\n",
    "    plt.scatter(X_train_lda[y_train == l, 0],\n",
    "                X_train_lda[y_train == l, 1] * (-1),\n",
    "                c=c, label=l, marker=m)\n",
    "\n",
    "plt.xlabel('LD 1')\n",
    "plt.ylabel('LD 2')\n",
    "plt.legend(loc='lower right')\n",
    "plt.tight_layout()\n",
    "# plt.savefig('images/05_08.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "\n",
    "lda4 = LDA(n_components=4)\n",
    "lda2 = LDA(n_components=2)\n",
    "X_train_lda4 = lda4.fit_transform(X_train_std, y_train)\n",
    "X_train_lda2 = lda2.fit_transform(X_train_std, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_lda2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression()\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "xgb_clf = XGBClassifier()\n",
    "\n",
    "model = xgb_clf\n",
    "\n",
    "model = model.fit(X_train_lda2, y_train)\n",
    "\n",
    "plot_decision_regions(X_train_lda2, y_train, classifier=model)\n",
    "plt.xlabel('LD 1')\n",
    "plt.ylabel('LD 2')\n",
    "plt.legend(loc='lower left')\n",
    "plt.tight_layout()\n",
    "# plt.savefig('images/05_09.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "xgb_clf = XGBClassifier()\n",
    "y_train_pred = cross_val_predict(xgb_clf, X_train_lda2, y_train, cv=4)\n",
    "conf_mx = confusion_matrix(y_train, y_train_pred)\n",
    "conf_mx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array([[ 237,  239,    5],\n",
    "       [  74, 1099,   21],\n",
    "       [  11,   74,   77]], dtype=int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_clf = XGBClassifier()\n",
    "y_train_pred = cross_val_predict(xgb_clf, X_train_std, y_train, cv=4)\n",
    "conf_mx = confusion_matrix(y_train, y_train_pred)\n",
    "conf_mx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame({'proba':[i[2] for i in probas], 'y_train_pred':predictions}, index=ready_dataset_for_marking.index)\n",
    "#df.sort_index(inplace=True)\n",
    "df[df.y_train_pred==2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_clf = XGBClassifier()\n",
    "xgb_clf.fit(X_train_lda2, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_t = scaler.transform(ready_dataset_for_marking)\n",
    "X_t = lda2.transform(X_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = xgb_clf.predict(X_t)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probas = xgb_clf.predict_proba(X_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ядерный анализ главных компонентов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import pdist, squareform\n",
    "from scipy import exp\n",
    "from scipy.linalg import eigh\n",
    "import numpy as np\n",
    "\n",
    "def rbf_kernel_pca(X, gamma, n_components):\n",
    "    \"\"\"\n",
    "    RBF kernel PCA implementation.\n",
    "\n",
    "    Parameters\n",
    "    ------------\n",
    "    X: {NumPy ndarray}, shape = [n_samples, n_features]\n",
    "        \n",
    "    gamma: float\n",
    "      Tuning parameter of the RBF kernel\n",
    "        \n",
    "    n_components: int\n",
    "      Number of principal components to return\n",
    "\n",
    "    Returns\n",
    "    ------------\n",
    "     X_pc: {NumPy ndarray}, shape = [n_samples, k_features]\n",
    "       Projected dataset   \n",
    "\n",
    "    \"\"\"\n",
    "    # Calculate pairwise squared Euclidean distances\n",
    "    # in the MxN dimensional dataset.\n",
    "    sq_dists = pdist(X, 'sqeuclidean')\n",
    "\n",
    "    # Convert pairwise distances into a square matrix.\n",
    "    mat_sq_dists = squareform(sq_dists)\n",
    "\n",
    "    # Compute the symmetric kernel matrix.\n",
    "    K = exp(-gamma * mat_sq_dists)\n",
    "\n",
    "    # Center the kernel matrix.\n",
    "    N = K.shape[0]\n",
    "    one_n = np.ones((N, N)) / N\n",
    "    K = K - one_n.dot(K) - K.dot(one_n) + one_n.dot(K).dot(one_n)\n",
    "\n",
    "    # Obtaining eigenpairs from the centered kernel matrix\n",
    "    # scipy.linalg.eigh returns them in ascending order\n",
    "    eigvals, eigvecs = eigh(K)\n",
    "    eigvals, eigvecs = eigvals[::-1], eigvecs[:, ::-1]\n",
    "\n",
    "    # Collect the top k eigenvectors (projected samples)\n",
    "    X_pc = np.column_stack((eigvecs[:, i]\n",
    "                            for i in range(n_components)))\n",
    "\n",
    "    return X_pc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = X_train_std, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.where(y==2,1,0)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len([i for i in y if i==2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "#from sklearn.datasets import make_moons\n",
    "\n",
    "#X, y = make_moons(n_samples=100, random_state=123)\n",
    "\n",
    "plt.scatter(X[y == 0, 0], X[y == 0, 1], color='red', marker='^', alpha=0.5)\n",
    "plt.scatter(X[y == 2, 0], X[y == 2, 1], color='blue', marker='o', alpha=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "# plt.savefig('images/05_12.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=np.where(y==0,0,1)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "scikit_pca = PCA(n_components=2)\n",
    "X_spca = scikit_pca.fit_transform(X)\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(7, 3))\n",
    "\n",
    "ax[0].scatter(X_spca[y == 0, 0], X_spca[y == 0, 1],\n",
    "              color='red', marker='^', alpha=0.5)\n",
    "ax[0].scatter(X_spca[y == 2, 0], X_spca[y == 2, 1],\n",
    "              color='blue', marker='o', alpha=0.5)\n",
    "\n",
    "#ax[1].scatter(X_spca[y == 0, 0], np.zeros((int(1838/2), 1)) + 0.02,\n",
    "              #color='red', marker='^', alpha=0.5)\n",
    "#ax[1].scatter(X_spca[y == 1, 0], np.zeros((int(1838/2), 1)) - 0.02,\n",
    "             # color='blue', marker='o', alpha=0.5)\n",
    "\n",
    "ax[0].set_xlabel('PC1')\n",
    "ax[0].set_ylabel('PC2')\n",
    "#ax[1].set_ylim([-1, 1])\n",
    "#ax[1].set_yticks([])\n",
    "#ax[1].set_xlabel('PC1')\n",
    "\n",
    "plt.tight_layout()\n",
    "# plt.savefig('images/05_13.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_kpca = rbf_kernel_pca(X, gamma=10, n_components=2)\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1,ncols=2, figsize=(12,5))\n",
    "ax[0].scatter(X_kpca[y==0, 0], X_kpca[y==0, 1], \n",
    "            color='red', marker='^', alpha=0.5)\n",
    "ax[0].scatter(X_kpca[y==2, 0], X_kpca[y==2, 1],\n",
    "            color='blue', marker='o', alpha=0.5)\n",
    "\n",
    "#ax[1].scatter(X_kpca[y==0, 0], np.zeros((50,1))+0.02, \n",
    "           # color='red', marker='^', alpha=0.5)\n",
    "#ax[1].scatter(X_kpca[y==1, 0], np.zeros((50,1))-0.02,\n",
    "          #  color='blue', marker='o', alpha=0.5)\n",
    "\n",
    "ax[0].set_xlabel('PC1')\n",
    "ax[0].set_ylabel('PC2')\n",
    "#ax[1].set_ylim([-1, 1])\n",
    "#ax[1].set_yticks([])\n",
    "#ax[1].set_xlabel('PC1')\n",
    "\n",
    "plt.tight_layout()\n",
    "# plt.savefig('images/05_14.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import MDS\n",
    "embedding = MDS(n_components=3, max_iter=100)\n",
    "X_transformed = embedding.fit_transform(X)\n",
    "X_transformed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1,ncols=2, figsize=(12,5))\n",
    "ax[0].scatter(X_transformed[y==0, 0], X_transformed[y==0, 1], \n",
    "            color='red', marker='^', alpha=0.5)\n",
    "ax[0].scatter(X_transformed[y==1, 0], X_transformed[y==1, 1],\n",
    "            color='blue', marker='o', alpha=0.5)\n",
    "\n",
    "#ax[1].scatter(X_kpca[y==0, 0], np.zeros((50,1))+0.02, \n",
    "           # color='red', marker='^', alpha=0.5)\n",
    "#ax[1].scatter(X_kpca[y==1, 0], np.zeros((50,1))-0.02,\n",
    "          #  color='blue', marker='o', alpha=0.5)\n",
    "\n",
    "ax[0].set_xlabel('PC1')\n",
    "ax[0].set_ylabel('PC2')\n",
    "#ax[1].set_ylim([-1, 1])\n",
    "#ax[1].set_yticks([])\n",
    "#ax[1].set_xlabel('PC1')\n",
    "\n",
    "plt.tight_layout()\n",
    "# plt.savefig('images/05_14.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Последовательный выбор признаков"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import clone\n",
    "from itertools import combinations\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "class SBS():\n",
    "    def __init__(self, estimator, k_features, scoring=accuracy_score,\n",
    "                 test_size=0.25, random_state=1):\n",
    "        self.scoring = scoring\n",
    "        self.estimator = clone(estimator)\n",
    "        self.k_features = k_features\n",
    "        self.test_size = test_size\n",
    "        self.random_state = random_state\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \n",
    "        X_train, X_test, y_train, y_test = \\\n",
    "            train_test_split(X, y, test_size=self.test_size,\n",
    "                             random_state=self.random_state)\n",
    "\n",
    "        dim = X_train.shape[1]\n",
    "        self.indices_ = tuple(range(dim))\n",
    "        self.subsets_ = [self.indices_]\n",
    "        score = self._calc_score(X_train, y_train, \n",
    "                                 X_test, y_test, self.indices_)\n",
    "        self.scores_ = [score]\n",
    "\n",
    "        while dim > self.k_features:\n",
    "            scores = []\n",
    "            subsets = []\n",
    "\n",
    "            for p in combinations(self.indices_, r=dim - 1):\n",
    "                score = self._calc_score(X_train, y_train, \n",
    "                                         X_test, y_test, p)\n",
    "                scores.append(score)\n",
    "                subsets.append(p)\n",
    "\n",
    "            best = np.argmax(scores)\n",
    "            self.indices_ = subsets[best]\n",
    "            self.subsets_.append(self.indices_)\n",
    "            dim -= 1\n",
    "\n",
    "            self.scores_.append(scores[best])\n",
    "        self.k_score_ = self.scores_[-1]\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return X[:, self.indices_]\n",
    "\n",
    "    def _calc_score(self, X_train, y_train, X_test, y_test, indices):\n",
    "        self.estimator.fit(X_train[:, indices], y_train)\n",
    "        y_pred = self.estimator.predict(X_test[:, indices])\n",
    "        score = self.scoring(y_test, y_pred)\n",
    "        return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "forest_clf = RandomForestClassifier(random_state=42)\n",
    "# selecting features\n",
    "sbs = SBS(forest_clf, k_features=10)\n",
    "sbs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sbs.subsets_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting performance of feature subsets\n",
    "k_feat = [len(k) for k in sbs.subsets_]\n",
    "\n",
    "plt.plot(k_feat, sbs.scores_, marker='o')\n",
    "plt.ylim([0.2, 1.02])\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Number of features')\n",
    "plt.grid()\n",
    "plt.tight_layout()\n",
    "# plt.savefig('images/04_08.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k3 = list(sbs.subsets_[18])\n",
    "#print(df_wine.columns[1:][k3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "k3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(md['md.columns'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_clf.fit(X_train, y_train)\n",
    "print('Training accuracy:', forest_clf.score(X_train, y_train))\n",
    "print('Test accuracy:', forest_clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_clf.fit(X_train[:, k3], y_train)\n",
    "print('Training accuracy:', forest_clf.score(X_train[:, k3], y_train))\n",
    "print('Test accuracy:', forest_clf.score(X_test[:, k3], y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Выявление гиперпараметров моделей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "xgb_clf = XGBClassifier()\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "forest_clf = RandomForestClassifier(random_state=32, class_weight='balanced')\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "gboost_clf = GradientBoostingClassifier()\n",
    "\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "bernoulli_clf = BernoulliNB()\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "logit = LogisticRegression(penalty='l1',multi_class='multinomial', solver='saga')\n",
    "\n",
    "\n",
    "\n",
    "#CATBOOST ХОРОШО СЕБЯ ПОКАЗАВШИЙ НЕ ЗАБЫТЬ\n",
    "\n",
    "#model=linear_model.LogisticRegression(penalty='l1', class_weight={False:0.55, True:0.45}, C=5).fit(X_train,y_train)\n",
    "#model = RandomForestClassifier(random_state=50, class_weight={0:0.26, 1:0.61, 2:0.13},n_estimators=100)\n",
    "#model = DecisionTreeClassifier(random_state=50,class_weight={False:0.1, True:0.9})\n",
    "#model = KNeighborsClassifier(n_neighbors=2)\n",
    "\n",
    "#params={'iterations' : 200, 'learning_rate' : 0.1,'eval_metric' : 'Accuracy', 'verbose' : True}#'class_weights':[0.4, 0.2, 0.4]}\n",
    "#cbc=CatBoostClassifier(**params)\n",
    "\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "svc = SVC(kernel='rbf', random_state=1, gamma=0.1, C=5)\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "tree = DecisionTreeClassifier(criterion='entropy', \n",
    "                              max_depth=5, \n",
    "                              random_state=1, class_weight='balanced')\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "ada = AdaBoostClassifier(base_estimator=tree, n_estimators=500, learning_rate=0.1, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=ada\n",
    "model.fit(X_train,y_train)\n",
    "preds=model.predict(X_test)\n",
    "accuracy_score(y_test, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = cross_val_predict(model, X_train_std, y_train, cv=4)\n",
    "conf_mx = confusion_matrix(y_train, y_train_pred)\n",
    "conf_mx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#param_grid = [{'class_prior': [[0.1, 0.8, 0.1], None], 'alpha': [0.01, 0.3, 0.6, 1.0]}]\n",
    "#param_grid = [{'learning_rate': [0.1], 'max_depth': [5] ,'max_features': [10], 'min_samples_split': [4,6,8]}]\n",
    "#param_grid = [{'max_depth': [8] ,'max_features': ['auto', 5, 10], 'min_samples_split': [2,6,8], 'n_estimators': [10,100], 'class_weight': [None, {0:0.1, 1:0.8, 2:0.1}] }]\n",
    "#param_grid = [{'max_depth': [3,5,8]}]\n",
    "#param_grid = [{'learning_rate': [0.2, 0.5]}]\n",
    "#param_grid = [{'C': [0.1, 1, 5], 'gamma': [0.1, 1, 10, 100]}]\n",
    "param_grid = [{'class_weight':[{0:0.26, 1:0.61, 2:0.13}, None], 'max_depth': [3,5,10] ,'max_features': [None, 10,50], 'min_samples_split': [2,6,8]}]\n",
    "\n",
    "grid_search = GridSearchCV(model, param_grid, cv=4,\n",
    "                           scoring='balanced_accuracy', return_train_score=True)\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = RandomForestClassifier(random_state=32, class_weight='balanced')\n",
    "#model = SVC(kernel='rbf', random_state=10, gamma=0.1, C=100)\n",
    "#model = RandomForestClassifier(random_state=52)\n",
    "model = DecisionTreeClassifier(criterion='gini', \n",
    "                              max_depth=2, \n",
    "                              random_state=10, class_weight='balanced')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array([[ 196,  284,    1],\n",
    "       [ 128, 1062,    4],\n",
    "       [  27,  133,    2]], dtype=int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_mx = confusion_matrix(y_train, y_train_pred)\n",
    "conf_mx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array([[ 237,  243,    1],\n",
    "       [ 106, 1085,    3],\n",
    "       [  25,  131,    6]], dtype=int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds=grid_search.best_estimator_.predict(X_test)\n",
    "accuracy_score(y_test, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "conf_mx = confusion_matrix(y_test, preds)\n",
    "conf_mx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydotplus import graph_from_dot_data\n",
    "from sklearn.tree import export_graphviz\n",
    "\n",
    "dot_data = export_graphviz(grid_search.best_estimator_, out_file=None) \n",
    "graph = graph_from_dot_data(dot_data) \n",
    "graph.write_png('tree.png') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pydotplus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gr = pydotplus.graphviz.graph_from_dot_data(dot_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Встраивание модели МО в веб-приложение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "\n",
    "dest = os.path.join('movieclassifier', 'pkl_objects')\n",
    "if not os.path.exists(dest):\n",
    "    os.makedirs(dest)\n",
    "\n",
    "pickle.dump(y_train_pred, open(os.path.join(dest, 'stopwords.pkl'), 'wb'), protocol=4)   \n",
    "pickle.dump(model, open(os.path.join(dest, 'classifier.pkl'), 'wb'), protocol=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile movieclassifier/vectorizer.py\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "import re\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "cur_dir = os.path.dirname(__file__)\n",
    "stop = pickle.load(open(\n",
    "                os.path.join(cur_dir, \n",
    "                'pkl_objects', \n",
    "                'stopwords.pkl'), 'rb'))\n",
    "\n",
    "def tokenizer(text):\n",
    "    text = re.sub('<[^>]*>', '', text)\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)',\n",
    "                           text.lower())\n",
    "    text = re.sub('[\\W]+', ' ', text.lower()) \\\n",
    "                   + ' '.join(emoticons).replace('-', '')\n",
    "    tokenized = [w for w in text.split() if w not in stop]\n",
    "    return tokenized\n",
    "\n",
    "vect = HashingVectorizer(decode_error='ignore',\n",
    "                         n_features=2**21,\n",
    "                         preprocessor=None,\n",
    "                         tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# РЕЗУЛЬТАТЫ на станд. датасете, со снижением размерности и с применением огран. перечня md_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bernoulli_clf = BernoulliNB() - параметры по умолчанию"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, cross_validation:\n",
    "array([[252, 111,  15],\n",
    "       [259, 501, 118],\n",
    "       [ 52,  76,  52]],"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gboost_clf = GradientBoostingClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{'learning_rate': 0.1,\n",
    " 'max_depth': 5,\n",
    " 'max_features': 10,\n",
    " 'min_samples_split': 8}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, cross_validation:\n",
    "array([[207, 161,  10],\n",
    "       [ 76, 779,  23],\n",
    "       [ 15, 105,  60]], dtype=int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "forest_clf = RandomForestClassifier(random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{'class_weight': None,\n",
    " 'max_depth': 8,\n",
    " 'max_features': 'auto',\n",
    " 'min_samples_split': 8,\n",
    " 'n_estimators': 10}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, cross_validation:\n",
    "array([[203, 168,   7],\n",
    "       [ 74, 793,  11],\n",
    "       [ 11, 119,  50]], dtype=int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "xgb_clf = XGBClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'max_depth': 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, cross_validation:\n",
    "array([[202, 170,   6],\n",
    "       [ 74, 784,  20],\n",
    "       [ 17, 113,  50]], dtype=int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model=CatBoostClassifier(**params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "params={'iterations' : 330, 'learning_rate' : 0.2,'eval_metric' : 'Accuracy', 'verbose' : True}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, cross_validation:\n",
    "array([[205, 168,   5],\n",
    "       [ 85, 772,  21],\n",
    "       [ 22, 113,  45]], dtype=int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame({'y_test':y_test, 'preds':preds})#, 'probas':[i for i in model.predict_proba(X_test)]})\n",
    "df[df.preds==2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Визуализация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://tproger.ru/translations/python-data-vizualization/\n",
    "#\n",
    "# ПОПРОБОВАТЬ ИСПОЛЬЗОВАТЬ ЭТО, ПО ССЫЛКЕ. БИБЛИОТЕКИ cufflinks plotly ИМПОРТИРОВАЛ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Конкретная визуализация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mindex = pd.read_excel('marked_dataset_old.xlsx')#, index_col='Date')\n",
    "mindex.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'y_test_pred':y_km}, index=mindex.index)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(mindex), len(y_km)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[:400]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask0 = (df.y_test_pred==0)\n",
    "mask1 = (df.y_test_pred==1)\n",
    "mask2 = (df.y_test_pred==2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.scatter(list(df[mask0].index), list(df[mask0]['y_test_pred']), c='red', label='data', alpha=0.85, s=6)\n",
    "plt.scatter(list(df[mask1].index), list(df[mask1]['y_test_pred']), c='green', label='data', alpha=0.85, s=6)\n",
    "plt.scatter(list(df[mask2].index), list(df[mask2]['y_test_pred']), c='blue', label='data', alpha=0.85, s=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Снижение размерности"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_list=[]\n",
    "for i in range(3,6,1):\n",
    "    pca = decomposition.PCA(n_components=i)\n",
    "    pca.fit(X_train)\n",
    "    Xtrain = pca.transform(X_train)\n",
    "    #Xtest=pca.transform(X_test)\n",
    "    #model = RandomForestClassifier(random_state=40, class_weight={False:0.85, True:0.15},n_estimators=100)\n",
    "    #model = model\n",
    "    y_train_pred = cross_val_predict(model, Xtrain, y_train, cv=4)\n",
    "    conf = confusion_matrix(y_train, y_train_pred)\n",
    "    \n",
    "    conf_list.append([conf[1][1], conf[0][1]])\n",
    "    \n",
    "    print(conf[2][2], conf[1][2], conf[0][2], 'components = ', i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Стекинг"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "xgb_clf = XGBClassifier(max_depth=5)\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "forest_clf = RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
    "            max_depth=8, max_features='auto', max_leaf_nodes=None,\n",
    "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "            min_samples_leaf=1, min_samples_split=8,\n",
    "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=None,\n",
    "            oob_score=False, random_state=42, verbose=0, warm_start=False)\n",
    "\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "gboost_clf = GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
    "              learning_rate=0.1, loss='deviance', max_depth=5,\n",
    "              max_features=10, max_leaf_nodes=None,\n",
    "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "              min_samples_leaf=1, min_samples_split=8,\n",
    "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
    "              n_iter_no_change=None, presort='auto', random_state=None,\n",
    "              subsample=1.0, tol=0.0001, validation_fraction=0.1,\n",
    "              verbose=0, warm_start=False)\n",
    "\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "bernoulli_clf = BernoulliNB()\n",
    "\n",
    "logit = LogisticRegression(penalty='l1',multi_class='multinomial', solver='saga')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [xgb_clf, forest_clf, gboost_clf, bernoulli_clf, logit]\n",
    "dict_predict = {}\n",
    "for model in models:\n",
    "    y_train_pred = cross_val_predict(model, X_train, y_train, cv=4, method='predict_proba')\n",
    "    dict_predict[str(model)[:9]+' proba_0'] = np.array([i[0] for i in y_train_pred])\n",
    "    dict_predict[str(model)[:9]+' proba_1'] = np.array([i[1] for i in y_train_pred])\n",
    "    dict_predict[str(model)[:9]+' proba_2'] = np.array([i[2] for i in y_train_pred])\n",
    "    \n",
    "    \n",
    "dict_predict['y_train'] = np.array(y_train)\n",
    "df_predictions = pd.DataFrame(dict_predict, index=y_train.index)\n",
    "df_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = df_predictions['y_train']\n",
    "X_pred = df_predictions.drop(columns=['y_train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "xgb_clf_pred = XGBClassifier()\n",
    "forest_clf_pred = RandomForestClassifier()\n",
    "gboost_clf_pred = GradientBoostingClassifier()\n",
    "logit = LogisticRegression(penalty='l1',multi_class='multinomial', solver='saga')\n",
    "\n",
    "models = [xgb_clf_pred, forest_clf_pred, gboost_clf_pred, logit]\n",
    "\n",
    "for model in models:\n",
    "    y_train_pred = cross_val_predict(model, X_pred, Y_pred, cv=4)\n",
    "    print(str(model)[:9])\n",
    "    print(confusion_matrix(Y_pred, y_train_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XGBClassi\n",
    "[[ 255  221    5]\n",
    " [ 125 1059   10]\n",
    " [  25  115   22]]\n",
    "RandomFor\n",
    "[[ 258  209   14]\n",
    " [ 170 1006   18]\n",
    " [  31  111   20]]\n",
    "GradientB\n",
    "[[ 258  215    8]\n",
    " [ 132 1049   13]\n",
    " [  30  110   22]]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "XGBClassi\n",
    "[[227 149   2]\n",
    " [ 86 827  12]\n",
    " [ 17  82  34]]\n",
    "RandomFor\n",
    "[[231 143   4]\n",
    " [113 790  22]\n",
    " [ 23  79  31]]\n",
    "GradientB\n",
    "[[222 152   4]\n",
    " [ 93 814  18]\n",
    " [ 19  80  34]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# По результатам стекинга (параметры моделей на последнем шаге по умолчанию) будем оптимизировать на последнем шаге RandomFor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "уже при формировании решения, не будет уже кросс-валид.\n",
    "сразу фит, сохранить модель, потом загрузить модель, потом предикт."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#forest_clf = RandomForestClassifier()\n",
    "#xgb_clf = XGBClassifier()\n",
    "gboost_clf = GradientBoostingClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gboost_clf.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#param_grid = [{'max_depth': [5] ,'max_features': [3,5,'auto'], 'min_samples_split': [6],\n",
    "#               'n_estimators': [50,100, 150], 'class_weight': [None, {0:0.26, 1:0.61, 2:0.13}] }]\n",
    "#param_grid = [{'max_depth': [30, 24, 18]}]\n",
    "param_grid = [{'learning_rate': [0.1, 0.3], 'max_depth': [3,5,8] ,'max_features': [None, 10], 'min_samples_split': [2,6,8]}]\n",
    "grid_search = GridSearchCV(gboost_clf, param_grid, cv=4,\n",
    "                           scoring='balanced_accuracy', return_train_score=True)\n",
    "grid_search.fit(X_pred, Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(bootstrap=True, class_weight={0:0.26, 1:0.61, 2:0.13}, criterion='gini',\n",
    "            max_depth=5, max_features=3, max_leaf_nodes=None,\n",
    "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "            min_samples_leaf=1, min_samples_split=6,\n",
    "            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=None,\n",
    "            oob_score=False, random_state=42, verbose=0, warm_start=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = GradientBoostingClassifier()\n",
    "#model = XGBClassifier()\n",
    "model = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "y_train_pred = cross_val_predict(model, X_pred, Y_pred, cv=4)\n",
    "conf_mx = confusion_matrix(Y_pred, y_train_pred)\n",
    "conf_mx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame({'Y_pred':Y_pred, 'y_train_pred':y_train_pred})\n",
    "df.sort_index(inplace=True)\n",
    "df[df.y_train_pred==2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обучение выбранных на шаге 1 и 2 моделей и сохранение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "xgb_clf = XGBClassifier(max_depth=5)\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "forest_clf = RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
    "            max_depth=8, max_features='auto', max_leaf_nodes=None,\n",
    "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "            min_samples_leaf=1, min_samples_split=8,\n",
    "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=None,\n",
    "            oob_score=False, random_state=42, verbose=0, warm_start=False)\n",
    "\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "gboost_clf = GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
    "              learning_rate=0.1, loss='deviance', max_depth=5,\n",
    "              max_features=10, max_leaf_nodes=None,\n",
    "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "              min_samples_leaf=1, min_samples_split=8,\n",
    "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
    "              n_iter_no_change=None, presort='auto', random_state=None,\n",
    "              subsample=1.0, tol=0.0001, validation_fraction=0.1,\n",
    "              verbose=0, warm_start=False)\n",
    "\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "bernoulli_clf = BernoulliNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_clf.fit(X_train, y_train)\n",
    "forest_clf.fit(X_train, y_train)\n",
    "gboost_clf.fit(X_train, y_train)\n",
    "bernoulli_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(xgb_clf, 'xgb_clf.pkl')\n",
    "joblib.dump(forest_clf, 'forest_clf.pkl')\n",
    "joblib.dump(gboost_clf, 'gboost_clf.pkl')\n",
    "joblib.dump(bernoulli_clf, 'bernoulli_clf.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "joblib.dump(scaler, 'scaler.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_clf_for_df_predictions = XGBClassifier()\n",
    "xgb_clf_for_df_predictions.fit(X_pred, Y_pred)\n",
    "joblib.dump(xgb_clf_for_df_predictions, 'xgb_clf_for_df_predictions.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Загрузка сохраненных моделей и их рекомендательная работа, на входе X_test и y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ready_dataset_for_marking = pd.read_excel('ready_dataset_for_marking_last.xlsx')\n",
    "ready_dataset_for_marking.tail(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scaler = joblib.load('scaler_best.pkl')\n",
    "X_real_test = scaler.transform(ready_dataset_for_marking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_clf_from_disk = joblib.load('xgb_clf_best.pkl')\n",
    "forest_clf_from_disk = joblib.load('forest_clf_best.pkl')\n",
    "gboost_clf_from_disk = joblib.load('gboost_clf_best.pkl')\n",
    "bernoulli_clf_from_disk = joblib.load('bernoulli_clf_best.pkl')\n",
    "xgb_clf_for_df_predictions_from_disk = joblib.load('xgb_clf_for_df_predictions_best.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [xgb_clf_from_disk, forest_clf_from_disk, gboost_clf_from_disk, bernoulli_clf_from_disk, logit]\n",
    "dict_predict = {}\n",
    "for model in models:\n",
    "    y_test_pred = model.predict_proba(X_real_test)\n",
    "    dict_predict[str(model)[:9]+' proba_0'] = np.array([i[0] for i in y_test_pred])\n",
    "    dict_predict[str(model)[:9]+' proba_1'] = np.array([i[1] for i in y_test_pred])\n",
    "    dict_predict[str(model)[:9]+' proba_2'] = np.array([i[2] for i in y_test_pred])\n",
    "    \n",
    "    \n",
    "#dict_predict['y_test'] = np.array(y_test)\n",
    "#df_predictions = pd.DataFrame(dict_predict, index=y_test.index)\n",
    "df_predictions = pd.DataFrame(dict_predict, index=ready_dataset_for_marking.index)\n",
    "df_predictions.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Y_pred = df_predictions['y_test']\n",
    "#X_pred = df_predictions.drop(columns=['y_test'])\n",
    "X_pred_test = df_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = xgb_clf_for_df_predictions_from_disk.predict(X_pred_test)\n",
    "#y_test_pred = grid_search.best_estimator_.predict(X_pred_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df=pd.DataFrame({'y_test':Y_pred, 'y_test_pred':y_test_pred})\n",
    "df=pd.DataFrame({'y_test':'-', 'y_test_pred':y_test_pred}, index=ready_dataset_for_marking.index)\n",
    "#df.sort_index(inplace=True)\n",
    "df[df.y_test_pred==2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Одна бумага сыграла роль в погрешности по точности.\n",
    "В общем, надо работать с 500-а бумагами."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Кластерный анализ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "X, y = make_blobs(n_samples=150, \n",
    "                  n_features=2, \n",
    "                  centers=3, \n",
    "                  cluster_std=0.5, \n",
    "                  shuffle=True, \n",
    "                  random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], \n",
    "            c='white', marker='o', edgecolor='black', s=50)\n",
    "plt.grid()\n",
    "plt.tight_layout()\n",
    "#plt.savefig('images/11_01.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "y = Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "km = KMeans(n_clusters=3, \n",
    "            init='random', \n",
    "            n_init=10, \n",
    "            max_iter=300,\n",
    "            tol=1e-04,\n",
    "            random_state=0)\n",
    "\n",
    "y_km = km.fit_predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[y_km == 0, 0],\n",
    "            X[y_km == 0, 1],\n",
    "            s=50, c='lightgreen',\n",
    "            marker='s', edgecolor='black',\n",
    "            label='cluster 1')\n",
    "plt.scatter(X[y_km == 1, 0],\n",
    "            X[y_km == 1, 1],\n",
    "            s=50, c='orange',\n",
    "            marker='o', edgecolor='black',\n",
    "            label='cluster 2')\n",
    "plt.scatter(X[y_km == 2, 0],\n",
    "            X[y_km == 2, 1],\n",
    "            s=50, c='lightblue',\n",
    "            marker='v', edgecolor='black',\n",
    "            label='cluster 3')\n",
    "plt.scatter(km.cluster_centers_[:, 0],\n",
    "            km.cluster_centers_[:, 1],\n",
    "            s=250, marker='*',\n",
    "            c='red', edgecolor='black',\n",
    "            label='centroids')\n",
    "plt.legend(scatterpoints=1)\n",
    "plt.grid()\n",
    "plt.tight_layout()\n",
    "#plt.savefig('images/11_02.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Distortion: %.2f' % km.inertia_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distortions = []\n",
    "for i in range(1, 11):\n",
    "    km = KMeans(n_clusters=i, \n",
    "                init='k-means++', \n",
    "                n_init=10, \n",
    "                max_iter=300, \n",
    "                random_state=0)\n",
    "    km.fit(X)\n",
    "    distortions.append(km.inertia_)\n",
    "plt.plot(range(1, 11), distortions, marker='o')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('Distortion')\n",
    "plt.tight_layout()\n",
    "#plt.savefig('images/11_03.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import cm\n",
    "from sklearn.metrics import silhouette_samples\n",
    "\n",
    "km = KMeans(n_clusters=3, \n",
    "            init='k-means++', \n",
    "            n_init=10, \n",
    "            max_iter=300,\n",
    "            tol=1e-04,\n",
    "            random_state=0)\n",
    "y_km = km.fit_predict(X)\n",
    "\n",
    "cluster_labels = np.unique(y_km)\n",
    "n_clusters = cluster_labels.shape[0]\n",
    "silhouette_vals = silhouette_samples(X, y_km, metric='euclidean')\n",
    "y_ax_lower, y_ax_upper = 0, 0\n",
    "yticks = []\n",
    "for i, c in enumerate(cluster_labels):\n",
    "    c_silhouette_vals = silhouette_vals[y_km == c]\n",
    "    c_silhouette_vals.sort()\n",
    "    y_ax_upper += len(c_silhouette_vals)\n",
    "    color = cm.jet(float(i) / n_clusters)\n",
    "    plt.barh(range(y_ax_lower, y_ax_upper), c_silhouette_vals, height=1.0, \n",
    "             edgecolor='none', color=color)\n",
    "\n",
    "    yticks.append((y_ax_lower + y_ax_upper) / 2.)\n",
    "    y_ax_lower += len(c_silhouette_vals)\n",
    "    \n",
    "silhouette_avg = np.mean(silhouette_vals)\n",
    "plt.axvline(silhouette_avg, color=\"red\", linestyle=\"--\") \n",
    "\n",
    "plt.yticks(yticks, cluster_labels + 1)\n",
    "plt.ylabel('Cluster')\n",
    "plt.xlabel('Silhouette coefficient')\n",
    "\n",
    "plt.tight_layout()\n",
    "#plt.savefig('images/11_04.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "km = KMeans(n_clusters=2,\n",
    "            init='k-means++',\n",
    "            n_init=10,\n",
    "            max_iter=300,\n",
    "            tol=1e-04,\n",
    "            random_state=0)\n",
    "y_km = km.fit_predict(X)\n",
    "\n",
    "plt.scatter(X[y_km == 0, 0],\n",
    "            X[y_km == 0, 1],\n",
    "            s=50,\n",
    "            c='lightgreen',\n",
    "            edgecolor='black',\n",
    "            marker='s',\n",
    "            label='cluster 1')\n",
    "plt.scatter(X[y_km == 1, 0],\n",
    "            X[y_km == 1, 1],\n",
    "            s=50,\n",
    "            c='orange',\n",
    "            edgecolor='black',\n",
    "            marker='o',\n",
    "            label='cluster 2')\n",
    "\n",
    "plt.scatter(km.cluster_centers_[:, 0], km.cluster_centers_[:, 1],\n",
    "            s=250, marker='*', c='red', label='centroids')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.tight_layout()\n",
    "#plt.savefig('images/11_05.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(X)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import pdist, squareform\n",
    "\n",
    "row_dist = pd.DataFrame(squareform(pdist(df, metric='euclidean')))\n",
    "row_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. correct approach: Condensed distance matrix\n",
    "from scipy.cluster.hierarchy import linkage\n",
    "row_clusters = linkage(pdist(df, metric='euclidean'), method='complete')\n",
    "pd.DataFrame(row_clusters,\n",
    "             columns=['row label 1', 'row label 2',\n",
    "                      'distance', 'no. of items in clust.'],\n",
    "             index=['cluster %d' % (i + 1) \n",
    "                    for i in range(row_clusters.shape[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. correct approach: Input sample matrix\n",
    "\n",
    "row_clusters = linkage(df.values, method='complete', metric='euclidean')\n",
    "pd.DataFrame(row_clusters,\n",
    "             columns=['row label 1', 'row label 2',\n",
    "                      'distance', 'no. of items in clust.'],\n",
    "             index=['cluster %d' % (i + 1)\n",
    "                    for i in range(row_clusters.shape[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import dendrogram\n",
    "\n",
    "# make dendrogram black (part 1/2)\n",
    "# from scipy.cluster.hierarchy import set_link_color_palette\n",
    "# set_link_color_palette(['black'])\n",
    "\n",
    "row_dendr = dendrogram(row_clusters, \n",
    "                       #labels=labels,\n",
    "                       # make dendrogram black (part 2/2)\n",
    "                       # color_threshold=np.inf\n",
    "                       )\n",
    "plt.tight_layout()\n",
    "plt.ylabel('Euclidean distance')\n",
    "#plt.savefig('images/11_11.png', dpi=300, \n",
    "#            bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Как он это делает. Удивительно работает метод кластеризации из библиотеки - AgglomerativeClustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "ac = AgglomerativeClustering(n_clusters=3, \n",
    "                             affinity='euclidean', \n",
    "                             linkage='complete')\n",
    "labels = ac.fit_predict(X)\n",
    "print('Cluster labels: %s' % labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(labels).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'labels':labels}, index=marked_dataset.index)\n",
    "df[df.labels==1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
